# Estimation {#sec-estimation}

**Goals**:

1. Explore how a likelihood function for a parameter changes for different data.

2. Convince yourself that maximizing the likelihood and maximizing the log-likelihood will give you the same estimate of $\theta$.

## Lab 2.1: Maximum Likelihood Estimation

In this subsection, we make a few plots of various likelihoods we have encountered so far. Note that the likelihood plot will change for different data, and, if you would like, you can put in different data vectors to see how the likelihood changes.

### MLE Example: Binomial Likelihood

The code below examines the likelihood of $p$ in a binomial setting with known $n$. The peak shown in the plot gives the value of $p$ that maximizes this likelihood.

```{r}
#| warning: false
#| output: false
library(tidyverse)

## get binomial likelihood for each value of p
## p is a vector of probabilities
## n is a single value for the sample size
## dat is a vector of data

get_binom_lik <- function(p, n, data_vec) {
  
  ## for each value of p, map through the dbinom function for
  ## each data point (and multiply the results at the end to obtain
  ## the likelihood)
  binom_lik <- map_dbl(p,
                       ~ dbinom(x = data_vec, size = n, prob = .x) |>
                         prod() ## like the capital Pi in formula
  )
  
  ## the function returns a vector of likelihoods for each
  ## candidate p (and will be a vector of the same length as p)
  return(binom_lik)
}
```

```{r}
#| output: false
p <- seq(0, 1, by = 0.001)
n <- 20
dat <- c(5, 4, 2, 1, 0, 1, 2)

binom_liks <- get_binom_lik(p = p, n = n, data_vec = dat)

lik_df <- tibble(p, binom_liks)
ggplot(data = lik_df, aes(x = p, y = binom_liks)) +
  geom_line() +
  labs(y = "Likelihood") +
  theme_minimal()
```

Secondly, let's graph the likelihood and the log-likelihood to try and convince ourselves that maximizing the log-likelihood is an equivalent approach to maximizing the likelihood:

```{r}
get_binom_log_lik <- function(p, n, data_vec) {
  
  ## for each value of p, map through the dbinom function for
  ## each data point (and multiply the results at the end to obtain
  ## the likelihood)
  binom_log_lik <- map_dbl(p,
                       ~ dbinom(x = data_vec, size = n, prob = .x,
                                log = TRUE) |>
                         sum() ## like the summation in formula
  )
  
  ## the function returns a vector of likelihoods for each
  ## candidate p (and will be a vector of the same length as p)
  return(binom_log_lik)
}
```

```{r}
#| output: false
p <- seq(0.01, 0.5, by = 0.001)
n <- 20
dat <- c(5, 4, 2, 1, 0, 1, 2)

binom_liks <- get_binom_lik(p = p, n = n, data_vec = dat)
binom_log_liks <- get_binom_log_lik(p = p, n = n, data_vec = dat)

lik_df <- tibble(p, binom_liks, binom_log_liks) |>
  pivot_longer(2:3, names_to = "lik_type", values_to = "L")
ggplot(data = lik_df, aes(x = p, y = L)) +
  geom_line() +
  facet_wrap(~ lik_type, scales = "free_y", nrow = 2) +
  labs(y = "Likelihood") +
  theme_minimal()
```

__Exercise__. Both the likelihood and the log likelihood plots given above are plots of the likelihood of $p$ given our observed data $5, 4, 2, 1, 0, 1, 2$. Suppose that, instead of the data given above, we actually observed the following data: $10, 9, 7, 6, 5, 6, 7$. How do you think the likelihood and log-likelihood plots would change given this new data? Draw a sketch of a possible likelihood and log-likelihood given this new observed data over the current plot.

__Exercise__. Use your knowledge from calculus as well as the plots given to explain why we can find the MLE by taking the derivative of the likelihood and setting the resulting equation equal to 0.

__Exercise__. We mentioned in class that instead of taking the derivative of the likelihood, it's easier to take the derivative of the log-likelihood. Explain why employing this strategy still yields the MLE using the plots above.


### MLE Example: Rayleigh Distribution

In class, we derived the MLE for the parameter $\theta$ in the probability model:

$$
f(x \vert \theta) = \frac{2x}{\theta} e^{-\frac{x^2}{\theta}},
$$

Here, we graph that likelihood for a sample of $n = 10$ data points.
 
```{r}
#| output: false

thetas <- seq(0, 30, length.out = 1000)  ## may need to adjust these limits
dat <- c(0.9, 1.5, 2, 4.2, 1.2, 5.2, 6.7, 2.1, 1.5, 2.3)

get_theta_lik <- function(theta, data_vec) {
  ## since there is no "dbinom" equivalent for the unnamed probability model
  ## we have to code f(x \vert \theta) "by hand":
  theta_lik <- map_dbl(theta,
                       ~ ((2 * data_vec / .x) * exp(-data_vec^2 / .x)) |>
                         prod()
  )
  
  return(theta_lik)
}
 
theta_liks <- get_theta_lik(theta = thetas, data_vec = dat)

lik_df <- tibble(thetas, theta_liks)
ggplot(data = lik_df, aes(x = thetas, y = theta_liks)) +
  geom_line() +
  labs(y = "Likelihood") +
  theme_minimal()
```

### MLE Example: Poisson

Finally, consider the likelihood of various values of $\lambda$, where the random sample of data is from a Poisson probability model.

```{r}
dat <- c(8, 0, 4, 9, 1)
lambdas <- seq(0, 10, length.out = 1000) ## may need to adjust these limits


get_poisson_lik <- function(lambda, data_vec) {
  
  poisson_lik <- map_dbl(lambda,
                         ~ dpois(x = data_vec, lambda = .x) |>
                           prod()
  )
  
  return(poisson_lik)
}

poisson_liks <- get_poisson_lik(lambda = lambdas, data_vec = dat)
lik_df <- tibble(lambdas, poisson_liks)
ggplot(data = lik_df, aes(x = lambdas, y = poisson_liks)) +
  geom_line() +
  labs(y = "Likelihood") +
  theme_minimal()
```

__Exercise__. Many of the most common likelihood functions are unimodal with a single peak (so, there is a single MLE). Draw a sketch of a likelihood function such that using our strategy of finding the MLE (taking the derivative and setting equal to 0) would __fail__.

__Exercise__. Many of the most common likelihood functions are unimodal with a single peak (so, there is a single MLE). Draw another sketch of a likelihood function such that using our strategy of finding the MLE (taking the derivative and setting equal to 0) would __fail__.

## Lab 2.2: MSE Consistency

In this subsection, we illustrate an example visualizing some of the mean square error results we derived in class as well as a couple of consistent estimators.

### MSE Graph

Here, we again the example where $X_1, X_2, \ldots, X_n$ are independent and identically distributed random variables with a $\text{Unif}(0, \theta)$ distribution. We now have three estimators: $\hat{\theta}_{MOM}$, $\hat{\theta}_{MLE}$, and $\hat{\theta}^*$. We have already derived both the MLE, the MOM estimator, and adjusted the MLE to make a new, unbiased estimator based on the MLE. We also calculated the MSE of each of our three candidate estimators.

Now, we make a plot of the MSE for $1 \leq n \leq 20$.

<!-- theta star is  xmax (n + 1) / n  -->
<!-- maybe move this plot to a lab-->

```{r}
#| warning: false
#| echo: true
#| output: true
library(tidyverse)
n <- 1:20
theta <- 1
mse_mom <- (1 / (3 * n)) * theta^2
mse_mle <- (2 / ((n + 1) * (n + 2))) * theta^2
mse_mle_unb <- (1 / ((n * (n + 2)))) * theta^2

mse_plot <- tibble::tibble(n, mse_mom, mse_mle, mse_mle_unb) |>
  pivot_longer(2:4, names_to = "estimator", values_to = "mse")
ggplot(data = mse_plot, aes(x = n, y = mse)) +
  geom_line(aes(colour = estimator)) +
  theme_minimal() +
  scale_colour_viridis_d()
```

__Exercise__. How do these results match your theoretical calculations? Which of the three estimators seems most preferable?

__Exercise__. Consider two generic estimators for a parameter $\theta$, and denote these estimators $\hat{\theta}_1$ and $\hat{\theta}_2$. Draw a plot similar to the plot given above ($n$ on x-axis, MSE on y-axis) that shows that $\hat{\theta}_1$ is preferable at some sample sizes but $\hat{\theta}_2$ is preferable for other sample sizes.

<br>

### Binomial Example Consistency

```{r Binomial}
#| warning: false
library(tidyverse)
p <- 1 / 4 ## specify probability of success for the simulation

nsim <- 1000 ## specify range of possible ns (how long we will run the simulation)

compute_binom_estimator <- function(p, nsim) {
  
  ## draw nsim different of 0's and 1's
  bernoulli_draws <- rbinom(n = nsim, size = 1, prob = p)

  ## compute the estimator of p
  ## nsim different times (once using a sample of 1 bernoulli, then
  ## using a sample of 2 bernoullis, ....,
  ##  then using a sample of nsim bernoullis)
  
  binom_mle <- map_dbl(1:nsim, ~ sum(bernoulli_draws[1:.x]) / .x)


  ## put all three results into a data frame, along with n
  output_df <- tibble(n = 1:nsim, binom_mle)
  
  return(output_df)
}

plot_df <- compute_binom_estimator(p = p, nsim = nsim)

ggplot(data = plot_df, aes(x = n,
                             y = binom_mle)) +
  geom_line() +
  geom_hline(yintercept = p, linetype = 2) +
  theme_minimal() +
  labs(x = "n (Sample Size)",
       caption = "Dotted Line Shows True Value of p")
```

__Exercise__. On the plot above, sketch in an estimator that does not exhibit evidence of consistency because the estimator does not seem to be asymptotically unbiased.

__Exercise__. In the provided lab code, change `nsim` to be equal to `50`. Does the MLE look like it converges to the true value of $p$? Explain why the resulting plot is not enough to confirm whether or not the estimator is consistent for $p$.

<br>

### Uniform Example Consistency

```{r Uniform}
#| output: false

theta <- 10  ## specify an upper bound for our uniform model
## recall that the lower bound is known and is equal to 0

nsim <- 1000 ## specify range of possible sample sizes (how long we will run the simulation)

## compute each estimator of theta using adjusted MLE, MLE, and MOM
compute_unif_estimators <- function(theta, nsim) {
  
  unif_draws <- runif(n = nsim, min = 0, max = theta)

  ## compute the method of moment estimator 
  ## nsim different times (once using a sample of size 1, then
  ## using a sample of size 2, ...., then using a sample of size nsim)
  unif_mom <- map_dbl(1:nsim, ~ 2 * mean(unif_draws[1:.x]))
  
  ## same type of computation for MLE
  unif_mle <- map_dbl(1:nsim, ~ max(unif_draws[1:.x]))
  
  ## same type of computation for adjusted MLE
  unif_mle_adj <- map_dbl(1:nsim,
                          ~ ((.x + 1) / .x) * max(unif_draws[1:.x]))

  ## put all three results into a data frame, along with n
  output_df <- tibble(n = 1:nsim, 
         unif_mom, unif_mle, unif_mle_adj)
  
  return(output_df)
}

plot_df <- compute_unif_estimators(theta = theta, nsim = nsim)
```

```{r}
#| output: false
## do some DATA/STAT 234-type Work to make the plot

plot_long <- plot_df |> pivot_longer(cols = starts_with("unif"),
                                     names_to = "Estimator",
                                     values_to = "theta_estimate")
ggplot(data = plot_long, aes(x = n,
                             y = theta_estimate,
                             colour = Estimator)) +
  geom_line() +
  geom_hline(yintercept = theta, linetype = 2) +
  scale_colour_viridis_d(end = 0.9) +
  theme_minimal() +
  labs(x = "n (Sample Size)",
       caption = "Dotted Line Shows True Value of Theta")
```

__Exercise__. On the plot above, sketch in an estimator that does not exhibit evidence of consistency because the estimator's asymptotic variance does not seem to go to 0. 

<br>

## Mini Project 2: A Meaningful Story

__AI Usage__: You may not use generative AI for this project in any way.

__Collaboration__: For this project, you may not work with other people in the class. Your story must be your own.

__Statement of Integrity__: Your submission should be typed. At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.

"All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project."

For this mini-project, you will write a "meaningful story." A "meaningful story" is one continuous piece of writing / creative work that uses key words from a list and in which the sentences "make sense and hang together." That is, the ideas in the story must illustrate that you understand key concepts from Stat 326 in a way that allows you to write "meaningfully" about them. You may not simply write ten sequential sentences that merely define the terms; the sentences must demonstrate relationships between the terms. It is your job to use the terms in a way that demonstrates that you understand the statistical concepts involved and why we care about these terms in the big picture of statistical theory.

In addition, you need to frame your writing within a real-life or imaginary context or scenario. Be creative!! Write about sports or music or manufacturing cell phones or skiing trips or the zombie apocalypse. Meaningful stories could even be literary works, such as play scripts, stories, song lyrics, poetry, etc. Use your imagination when constructing your "story" and conveying the material (content MUST be appropriate).

__Estimation Prompt__: Each of these terms must be incorporated into your meaningful story. Estimator, Parameter, Estimate (as a noun), Random Variable, Random Sample, Bias, Variance, Consistent, Likelihood.

__Also__: in your "meaningful story," you must refer to at least one of our common probability distributions by name.



