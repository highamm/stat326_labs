# Hypothesis Testing {#sec-hyptest}

**Goals**:

* run a permutation test on a couple of different data sets to assess two competing hypotheses.
* use empirical power to compute the approximate power of a test under a few different assumptions.

## Lab 5.1: Permutation Tests

In this lab, we use permutation tests to assess whether there is evidence that the mean delay times of two different airlines are different and whether there is evidence that completing a difficult task with a dog in the room lowers stress level, on average.

### Example 1: Flight Data

Consider again the flight delay data. Here, we will consider whether or not there is statistical evidence that the mean delay of United Airlines is different than the mean delay of American Airlines. 

Recall that, for a permutation test, we do not need to assume that the underlying populations follow a normal distribution. However, if we are interested in testing a difference in means, we do need to assume that the populations have the same variance and the same shape (but perhaps may have a different center). Note that, if we have a randomized experiment, then these assumptions can be relaxed.

First, we obtain some summary statistics and make a plot of the data:

```{r}
#| warning: false
library(tidyverse)
library(resampledata)
delay_df <- FlightDelays |> as_tibble() |>
  select(Carrier, Delay, everything())
## set.seed(13617)  # 50100 for bias illustration

delay_df |> group_by(Carrier) |>
  summarise(n = n(),
            xbar = mean(Delay),
            sd = sd(Delay))

ggplot(delay_df,
       aes(x = Carrier, y = Delay)) + 
  geom_boxplot(fill = "steelblue") +
  theme_minimal()
```

We next store the difference in sample means as a value called `teststat` and each of the sample sizes as `n_a` and `n_u`:

```{r}
teststat <- delay_df |> group_by(Carrier) |>
  summarise(mean_delay = mean(Delay)) |>
  pull(mean_delay) |>
  diff()
## teststat is united delay mean minus american airlines mean
```

Under the null hypothesis, the distribution of delays is identical for american and united airlines. To reshuffle the delay values randomly across both airlines we can use the code below:

```{r}
delay_df |>
  mutate(delay_perm = sample(Delay, size = nrow(delay_df), replace = FALSE)) |>
  relocate(delay_perm)
```

Rerun the previous code chunk a few times to see a different permutations of the `Delay` variable.

With one permutation, we want to recalculate the difference in means:

```{r}
delay_df |>
  mutate(delay_perm = sample(Delay, size = nrow(delay_df), replace = FALSE)) |>
  relocate(delay_perm) |>
  group_by(Carrier) |>
  summarise(mean_delay_perm = mean(delay_perm)) |>
  pull(mean_delay_perm) |>
  diff()
```

Rerun the code chunk above a few times to get a few different mean differences under different permutations. The big idea is that a large number of these mean differences will form the null distribution for the difference in sample means (if there really is no difference in the underlying population distributions).

So, let's wrap that above code in a function, and then iterate over that function a large number of times to obtain our null distribution:

```{r}
get_delay_perm_diff <- function() {
  delay_df |>
    mutate(delay_perm = sample(Delay, size = nrow(delay_df), replace = FALSE)) |>
    relocate(delay_perm) |>
    group_by(Carrier) |>
    summarise(mean_delay_perm = mean(delay_perm)) |>
    pull(mean_delay_perm) |>
    diff()
}

n_perm <- 5000
diff_means <- map_dbl(1:n_perm, \(i) get_delay_perm_diff())
null_df <- tibble(diff_means)

ggplot(data = null_df, aes(x = diff_means)) +
  geom_histogram(colour = "orange4", fill = "orange2", bins = 15) +
  theme_minimal()
```

__Exercise__. Write a 1-2 sentence interpretation of what this null distribution means in context of the problem.

__Exercise__. Write a 1-2 sentence explanation on what the code above is doing.

Now, let's add where our test statistic from our data is on the graph:

```{r}
ggplot(data = null_df, aes(x = diff_means)) +
  geom_histogram(colour = "orange4", fill = "orange2", bins = 15) +
  geom_vline(xintercept = teststat, colour = "grey75") +
  theme_minimal()
```

__Exercise__. Before explicitly calculating a p-value, assess visually whether a p-value for this hypothesis test will be relatively large or relatively small.

Now, let's explicitly compute a conservative p-value for the test:

```{r}
(sum(diff_means > abs(teststat)) + sum(diff_means < -abs(teststat)) + 1) / (n_perm + 1)
```

__Exercise__. What is the alternative hypothesis that the p-value is testing?

__Exercise__. What is the purpose of the `+ 1` in the code above?

__Exercise__. Why are there two different summations in the code above to get the p-value?

__Exercise__. Write a conclusion in context of the problem for this hypothesis test.

### Example 2: Stress Levels

```{r}
#| echo: false
#| warning: false
#| output: false
#| eval: false
library(tidyverse)
stress_df <- read_csv(here::here("data/stress.csv")) |> 
  filter(group == "P" | group == "C")
write_csv(stress_df, here::here("data/stress_dog.csv"))
```

Many of you took our probability exams with the adorable Mipha providing moral support throughout the exam. There has actually been formal research assessing whether being in the presence of a dog can have beneficial effects for people. In one such study, researchers recruited 30 women who were self-proclaimed dog lovers. They randomly assigned 15 women to a stressful task alone (control: `C`) and 15 women to do a stressful task with a pet dog present (pet: `P`). The response variable is heart rate during the task, with higher heart rates presumed to mean that the there was more stress during the task.

Use the code below to read in the data and do some brief data exploration:

```{r}
library(tidyverse)
stress_df <- read_csv("https://raw.githubusercontent.com/highamm/stat326_labs/master/data/stress_dog.csv")
```

__Data Exploration__:

```{r}
ggplot(data = stress_df, aes(x = group, y = rate, fill = group)) +
  geom_boxplot(outlier.shape = 8) +
  theme_minimal() +
  scale_fill_viridis_d(begin = 0.4, end = 0.9) +
  guides(fill = "none")

stress_df |> group_by(group) |>
  summarise(mean = mean(rate),
            n = n())
```

__Exercise__. What do you find from your data exploration plot? What are the observed sample mean heart rates for each group?

<br>

__Test Statistic__:

```{r}
# From our summary statistics
teststat <- stress_df |> group_by(group) |>
  summarise(mean = mean(rate),
            n = n()) |>
  pull(mean) |>
  diff()
## Pet minus Control
teststat
```

__Exercise__: Modify the following code to create the null distribution for a test that the mean heart rate is different in each of the two groups.

```{r}
#| eval: false
get_stress_perm_diff <- function() {
  stress_df |>
    mutate(rate_perm = sample(____,
                                size = nrow(____), replace = FALSE)) |>
    relocate(rate_perm) |>
    group_by(_______) |>
    summarise(mean_rate_perm = mean(rate_perm)) |>
    pull(mean_rate_perm) |>
    diff()
}
get_rate_perm_diff()

n_perm <- 5000
diff_means <- map_dbl(1:n_perm, \(i) get_rate_perm_diff())
null_df <- tibble(diff_means)

ggplot(data = null_df, aes(x = diff_means)) +
  geom_histogram(colour = "skyblue4", fill = "skyblue1", bins = 15) +
  ## add test stat to null dist
  geom_vline(xintercept = teststat, colour = "purple") +
  theme_minimal()
```

__Exercise__. Calculate a p-value for the test.

```{r}
#| echo: false
#| output: false
(sum(diff_means > abs(teststat)) + sum(diff_means < -abs(teststat)) + 1) / (n_perm + 1)
```

__Exercise__. Write a conclusion in context of the problem for this hypothesis test.

<br>

## Lab 5.2 Empirical Power

should have enough time for this

a short simulation study on the effects of different alt. hypothesis parameter values and the effects of sample size on power.

<br>

## Mini Project 5: Advantages and Drawbacks of Using p-values

__AI Usage__: You may not use generative AI for this project in any way.

__Collaboration__: For this project, you may work with a self-contained group of 3. Keep in mind that you may not work with the same person on more than one mini-project (so, if you worked with a student on the first, third, or fourth mini-projects as a partner or in a small group, you may not work with that person on this project). Finally, if working with a partner or in a group of 3, your write-ups must be distinct and written individually (so you can chat with your group about what you might say, but you should never copy a group member's response).

__Statement of Integrity__: At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.

"All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project."

<br>

For this final mini-project, you will read an editorial published in _The American Statistician_ and respond to the questions below. You will find a pdf of the editorial on Canvas.

First, read through the first six sections of the editorial titled _Moving to a World Beyond 'p<0.05'_ by Wasserstein, Schirm, and Lazar published in _The American Statistician_ in 2019 as the introduction to a special issue about p-values. I strongly encourage you to read the first six sections at least twice before you attempt to respond to the questions that appear below. This mini-project will be graded, primarily, on how thoughtfully you respond to these questions.

__Submission__: Upload your typed responses to these questions to the assignment in Canvas by the deadline. 

__Questions__:

1. _Towards the end of Section 1_, the authors say "As 'statistical significance' is used less, statistical thinking will be used more." Elaborate on what you think the authors mean. Give some examples of what you think embodies "statistical thinking."

2. _Section 2, third paragraph_: The authors state "A label of statistical significance adds nothing to what is already conveyed by the value of $p$; in fact, this dichotomization of p-values makes matters worse." Elaborate on what you think the authors means.

3. _Section 2, end of first column_: The authors state "For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight." Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?

4. _Section 3, end of page 2_: The authors state "The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation." Do you agree or disagree? Explain.

5. _Section 3.2_: The authors note that they are envisioning "a sort of 'statistical thoughtfulness'." What do you think "statistical thoughtfulness" means? What are some ways to demonstrate "statistical thoughtfulness" in an analysis?

6. _Section 3.2.4_: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as "significance" and "confidence" can be misleading, and they propose the use of "compatibility" instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?

7. Find a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?


