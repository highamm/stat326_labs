[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 326 Labs and Projects",
    "section": "",
    "text": "Site Information\nWelcome! This site contains code and exercises for the labs in STAT 326 (Mathematical Statistics), information for the mini projects in the course, and information about the final portfolio. While we will also do plenty of handwritten work to help understand concepts, the primary purpose of this site is to hold all of the computing/coding work in one place.\nThe syllabus for the entire course is also given here.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "STAT 326 Labs and Projects",
    "section": "Instructor Information",
    "text": "Instructor Information\n\nProfessor: Matt Higham\nOffice: Bewkes 123\nEmail: mhigham@stlawu.edu\nOffice Hours: 15 minute slots bookable at https://calendly.com/mhigham/prof-higham-office-hours. The link to book is also on our Canvas home page.\n\nNote that you must book a time for office hours at least 12 hours in advance to guarantee that I am present and available at that time.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "STAT 326 Labs and Projects",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse Notes\n\nPrint out and bring to class.\nFolder or 3-ring binder to keep notes in.\n\nLaptop",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#general-course-outcomes",
    "href": "index.html#general-course-outcomes",
    "title": "STAT 326 Labs and Projects",
    "section": "General Course Outcomes",
    "text": "General Course Outcomes\nADD COURSE OUTCOMES",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#use-of-r-and-rstudio",
    "href": "index.html#use-of-r-and-rstudio",
    "title": "STAT 326 Labs and Projects",
    "section": "Use of R and RStudio",
    "text": "Use of R and RStudio\nWe will use RStudio throughout the semester as a tool to help us understand concepts in this course.\n\nR and RStudio are both free to use.\nPart of your first homework assignment will give some direction on how to install these on your own computer.\nThe purpose of this class is not to learn R in detail. The concepts for the course receive a strong priority.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#exams",
    "href": "index.html#exams",
    "title": "STAT 326 Labs and Projects",
    "section": "Exams",
    "text": "Exams\n\nThere will be three midterm exams, each worth 150 points. The midterm exams will all take place in the evening at 6:30 pm. The tentative dates for the three midterms are: September 25 (Wednesday), October 23 (Wednesday), and December 4 (Wednesday). More information about the midterm exams will be given closer to the date of the first exam.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#quizzes",
    "href": "index.html#quizzes",
    "title": "STAT 326 Labs and Projects",
    "section": "Quizzes",
    "text": "Quizzes\nFor most weeks during the semester, we will have an in-person quiz that covers the material from the last few days of class, given on Wednesdays. The quiz will be similar to questions from that week’s homework assignment or to questions on a recent handout. You may use a sheet that we will make together in class that contains useful R functions and some useful formalas (so please do not memorize any formulas, distributions, or R functions!!).\n\nThe first three quizzes are worth 15 points each. The remaining seven quizzes are 30 points each and, in addition to containing a question covering new material, they will also contain “cumulative questions.” The general topic of these cumulative questions will be given in advance to help you prepare.\nYou may skip one (but only one) quiz during the semester (two of the first three quizzes can count as a single “drop”) so the total number of points for the quizzes is 225 points.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#homework-assignments",
    "href": "index.html#homework-assignments",
    "title": "STAT 326 Labs and Projects",
    "section": "Homework Assignments",
    "text": "Homework Assignments\nWeekly homeworks are worth 10 points each and are due nearly every week on Wednesdays. The structure of the homework is:\n\nyou should submit a signed statement to Canvas that you have attempted each part to each homework exercise. After you submit this signed statement, a file of partial solutions will be released that you can use to check your work. Make sure to submit this so that you have enough time to grade your homework and make any necessary corrections!\nby Wednesday’s class time, you should submit a hard copy of your corrected homework with your quiz for that day. A couple of notes for this submission:\n\nyou should not erase your original work: it’s helpful for me to see this work, and it’s helpful for you to have it as well as you prepare for the exams.\nyou must make your corrections in a different colour of ink. Again, the reason for this is that it helps me see your thought process and it helps you see what you had made errors on when you go to study for the exam.\nyou will get full points for the homework if you had fully attempted each problem before checking with the solutions and that you corrected any problems for which there were mistakes.\n\n\n\n\nYou may work with other students on the homeworks, but please make sure to read the Rules for Collaboration section before doing so. If you need additional help outside of collaboration with classmate or office hours, the Peterson Quantitative Research Center (PQRC) in Valentine Hall is a great resource!\nThere will be 11 homeworks throughout the semester, and your lowest homework score will be dropped. So, the total number of points for homeworks is 100 points.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#prep-tasks",
    "href": "index.html#prep-tasks",
    "title": "STAT 326 Labs and Projects",
    "section": "Prep Tasks",
    "text": "Prep Tasks\nIn addition to homework exercises, you will also be assigned “prep tasks” to complete by the start of class every Monday. While the purpose of homework is generally to practice concepts that we have already discussed in class, the purpose of the prep tasks is generally to introduce you to a new topic and give a couple of easier practice problems with that topic so that we can spend more of class time completing more challenging problems. There will be 20 prep tasks throughout the semester, each at 5 points for a total of 100 points.\nBecause these prep tasks are meant to be low-stakes introductions to new topics, you may use more than one attempt on 3 different 5-point prep tasks throughout the semester. How you use these are entirely up to you, but the additional attempts on the prep tasks must be made prior to that task’s deadline.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#participation",
    "href": "index.html#participation",
    "title": "STAT 326 Labs and Projects",
    "section": "Participation",
    "text": "Participation\nWe will use small groups to complete handouts and practice problems in-class throughout the semester. The groups will only be used for in-class work, and we will reassign groups a few times during the semester. Your grade for participation will be based on an evaluation through a rubric that we will complete three times throughout the semester. The rubric will be given to you on the first day of class so you know right away how you will be assessed for this portion of your grade.\nAdditionally, once during the semester, you will be tasked with coming up with a “warm-up” exercise for the class to complete at the start of class. More details about this will be given later.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "STAT 326 Labs and Projects",
    "section": "Final Project",
    "text": "Final Project\nInstead of a cumulative final exam, we will be having cumulative quizzes throughout the latter two-thirds of the semester. However, we will still have a small final project. More details about the nature of this project will be provided at the end of the semester.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#point-allocation",
    "href": "index.html#point-allocation",
    "title": "STAT 326 Labs and Projects",
    "section": "Point Allocation",
    "text": "Point Allocation\nThe 1000 points possible for the class will be allocated in the following way:\n\nExams: 450 points: 150 points for each of 3 midterm exams.\nQuizzes: 225 points: 7 30-point Quizzes and 3 15-point Quizzes (with one of the 30-point quizzes or the sum of two 15-point quizzes dropped).\nHomework Assignments: 100 points: 11 Homeworks for 10 points each for a total of 100 points (with one of the 10-point homeworks dropped).\nPrep Tasks: 100 points: 20 prep tasks, each worth 5 points.\nParticipation: 60 points: 3 participation rubric evaluations at 15 points each plus one warm-up exercise and solution worth 15 points.\nFinal Project: 65 points: 1 final project.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#grading-scale",
    "href": "index.html#grading-scale",
    "title": "STAT 326 Labs and Projects",
    "section": "Grading Scale",
    "text": "Grading Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n4.0\n3.75\n3.5\n3.25\n3.0\n2.75\n2.5\n2.25\n2.0\n1.75\n1.5\n1.25\n1.0\n0.0\n\n\n\n\nPoints\n950-1000\n920-949\n890-919\n860-889\n830-859\n810-829\n770-809\n750-769\n720-749\n700-719\n670-699\n640-669\n600-639\n0-599",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#rules-for-collaboration",
    "href": "index.html#rules-for-collaboration",
    "title": "STAT 326 Labs and Projects",
    "section": "Rules for Collaboration",
    "text": "Rules for Collaboration\nYou are allowed to collaborate with your classmates (or your classmates from the other section of this course) for homework assignments and handouts with the following rules.\n\nyou must state the name(s) of who you collaborated with at the top of each assessment.\nall work must be your own. Even if you work with someone else, you must write your answers on your own. Therefore, I expect your answers to free response questions to be at least slightly different from the person(s) you collaborated with.\nyou must not copy answers directly from the Internet or directly from the homework solutions file.\nthis isn’t a rule, but keep in mind that collaboration is not permitted on any of the exams. Therefore, when working with someone, make sure that you are both really learning so that you both can have success on the exam assessments.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#ai-policy",
    "href": "index.html#ai-policy",
    "title": "STAT 326 Labs and Projects",
    "section": "AI Policy",
    "text": "AI Policy\nThroughout the semester, we will use generative AI (specifically, ChatGPT) to aid us in performing basic calculations and in evaluating integrals. For homework assignments, the policy on AI usage will be stated at the top of the assignment. Make sure that you follow this policy. While AI can be a tool to enhance your learning, becoming overly reliant on it prevents the growth of logical thinking skills.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#diversity-statement",
    "href": "index.html#diversity-statement",
    "title": "STAT 326 Labs and Projects",
    "section": "Diversity Statement",
    "text": "Diversity Statement\nDiversity encompasses differences in age, colour, ethnicity, national origin, gender, physical or mental ability, religion, socioeconomic background, veteran status, sexual orientation, and marginalized groups. The interaction of different human characteristics brings about a positive learning environment. Diversity is both respected and valued in this classroom.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#accessibility-statement",
    "href": "index.html#accessibility-statement",
    "title": "STAT 326 Labs and Projects",
    "section": "Accessibility Statement",
    "text": "Accessibility Statement\nThe message below is copied from the Student Accessibility Services Office:\nYour experience in this class is important to me. It is the policy and practice of St. Lawrence University to create inclusive and accessible learning environments consistent with federal and state law. If you have established accommodations with the Student Accessibility Services Office in the past, please activate your accommodations so we can discuss how they will be implemented in this course.\nIf you have not yet established services through the Student Accessibility Services Office but have a temporary health condition or permanent disability that requires accommodations (conditions include but not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), please contact the Student Accessibility Services Office directly to set up a meeting. The Student Accessibility Services Office will work with you on the interactive process that establishes reasonable accommodations.\nColor Vision Deficiency: The Student Accessibility Services office can loan glasses for students who are color vision deficient. Please contact the office to make an appointment.\nFor more specific information about setting up an appointment with Student Accessibility Services please see the options listed below:\nTelephone: 315.229.5537\nEmail: studentaccessibility@stlawu.edu\nWebsite: https://www.stlawu.edu/offices/student-accessibility-services",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "STAT 326 Labs and Projects",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic dishonesty will not be tolerated. Any specific policies for this course are supplementary to the\nHonor Code. According to the St. Lawrence University Academic Honor Policy,\n\nIt is assumed that all work is done by the student unless the instructor/mentor/employer gives specific permission for collaboration.\nCheating on examinations and tests consists of knowingly giving or using or attempting to use unauthorized assistance during examinations or tests.\nDishonesty in work outside of examinations and tests consists of handing in or presenting as original work which is not original, where originality is required.\n\nClaims of ignorance and academic or personal pressure are unacceptable as excuses for academic dishonesty. Students must learn what constitutes one's own work and how the work of others must be acknowledged.\nFor more information, refer to www.stlawu.edu/acadaffairs/academic_honor_policy.pdf.\nTo avoid academic dishonesty, it is important that you follow all directions and collaboration rules and ask for clarification if you have any questions about what is acceptable for a particular assignment or exam. If I suspect academic dishonesty, a score of zero will be given for the entire assignment in which the academic dishonesty occurred for all individuals involved and Academic Honor Council will be notified. If a pattern of academic dishonesty is found to have occurred, a grade of 0.0 for the entire course can be given.\nIt is important to work in a way that maximizes your learning. Be aware that students who rely too much on others for the homework and projects tend to do poorly on the quizzes and exams.\nPlease note that in addition the above, any assignments in which your score is reduced due to academic dishonesty will not be dropped according to the homework policy e.g., if you receive a zero on a homework because of academic dishonesty, it will not be dropped from your grade.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#pqrc",
    "href": "index.html#pqrc",
    "title": "STAT 326 Labs and Projects",
    "section": "PQRC",
    "text": "PQRC\nThe Peterson Quantitative Resource Center (PQRC) offers free, no appointment necessary peer tutoring across a range of courses with quantitative content. The PQRC student staff of mentors is trained to assist students to develop and to improve their quantitative skills and understanding. More information about the PQRC’s current hours and modes of operation can be found at the PQRC webpage: https://www.stlawu.edu/offices/pqrc",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#tentative-calendar-of-due-dates",
    "href": "index.html#tentative-calendar-of-due-dates",
    "title": "STAT 326 Labs and Projects",
    "section": "Tentative Calendar of Due Dates",
    "text": "Tentative Calendar of Due Dates\nThe Exam Dates are tentatively scheduled for:\n\nExam 1: Wednesday, September 25 at 6:30 pm.\nExam 2: Wednesday, October 23 at 6:30 pm or Wednesday, October 30 at 6:30 pm.\nExam 3: Wednesday, November 20 at 6:30 pm or Wednesday, December 4 at 6:30 pm.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "samp-dist.html",
    "href": "samp-dist.html",
    "title": "1  Sampling Distributions",
    "section": "",
    "text": "Lab 1.1: Introduction to Statistical Simulation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#lab-1.1-introduction-to-statistical-simulation",
    "href": "samp-dist.html#lab-1.1-introduction-to-statistical-simulation",
    "title": "1  Sampling Distributions",
    "section": "",
    "text": "Starting a Simulation\nTo begin a simulation for a sampling distribution of a sample statistic, we need to choose:\n\na population model for the simulation. Let’s start with Normal(\\(\\mu\\) = 10, \\(\\sigma^2\\) = 4).\na sample size for the simulation. Let’s start with \\(n\\) = 5.\na calculation for the sample statistic that we are constructing the sampling distribution of. Let’s start with the sample mean, \\(\\bar{y}\\).\nGenerating a Single Sample Statistic\nCarefully look through this code and output to understand the process of generating a single sample from a population and computing a statistic.\nIn the code below, we simulate five observations from a normal population with mean 10 and standard deviation 2. Note that, when you run the code, you should get a different set of 5 numbers than the ones printed below: it is a random sample, after all!\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 2   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n# look at the sample\nsingle_sample \n#&gt; [1] 11.30 11.59  6.02  4.87 12.37\n\nNext, we compute the sample mean from this sample: this is our sample statistic we are interested in.\n\n# compute the sample mean\nsample_mean &lt;- mean(single_sample)\n# look at the sample mean\nsample_mean     \n#&gt; [1] 9.23\n\nAgain, your sample mean should be different!\nFinally, we can make a plot of our single sample, along with where the sample mean lies.\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  ## add the sample points from your sample\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  ## add a line for the sample mean\n  geom_vline(xintercept = sample_mean, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 2\")\n\n\n\n\n\n\n\nConstructing the Sampling Distribution\nTo simulate the sampling distribution of the sample mean from a normal population with \\(\\mu\\) = 10 and \\(\\sigma\\) = 2 for a sample size of 5, we need to repeat the above steps many, many, many times. We can do so by\n\nWriting a function that computes the sample mean and then\nMapping through that function a large number of times and then\nPlotting the large number of sample means to examine the characteristics of the resulting distribution.\n\nFirst, let’s write the function that will compute the sample mean with a given sample size from a normal population model with a given mean and standard deviation.\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_normal_mean &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_mean &lt;- mean(single_sample)\n  \n  return(sample_mean)\n}\n\n## test function once:\ngenerate_normal_mean(mu = mu, sigma = sigma, n = n)\n#&gt; [1] 10.67211\n\nNext, to generate 5000 sample means, we map through the function:\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_normal_mean function\n## nsim times\nmeans &lt;- map_dbl(1:nsim, \\(i) generate_normal_mean(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmeans_df &lt;- tibble(means)\nmeans_df\n#&gt; # A tibble: 5,000 × 1\n#&gt;   means\n#&gt;   &lt;dbl&gt;\n#&gt; 1 10.9 \n#&gt; 2 10.5 \n#&gt; 3 10.3 \n#&gt; 4  9.34\n#&gt; 5  9.27\n#&gt; 6 11.1 \n#&gt; # ℹ 4,994 more rows\n\nFinally, we plot the 5000 sample means to see what our sampling distribution of the sample mean (for a sample size of 5) looks like.\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nWe can also obtain some summary statistics of the sampling distribution of the sample mean when \\(n\\) = 5:\n\nmeans_df |&gt;\n  summarise(mean_samp_dist = mean(means),\n            var_samp_dist = var(means),\n            sd_samp_dist = sd(means))\n#&gt; # A tibble: 1 × 3\n#&gt;   mean_samp_dist var_samp_dist sd_samp_dist\n#&gt;            &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1           9.99         0.772        0.878\n\nAnd, we can even obtain estimates of the probability that we observe a sample mean larger than 11 by calculating the proportion of our observed sample means that are larger than 11. We can use a similar strategy to estimate the probability that we observe a sample mean less than or equal to 9.5.\n\n# What is the probability that we observe a sample mean larger than 11?\nmeans_df |&gt;\n  mutate(more_than_11 = if_else(means &gt; 11,\n                                true = 1, false = 0)) |&gt;\n  summarise(prob_more_than_11 = mean(more_than_11))\n#&gt; # A tibble: 1 × 1\n#&gt;   prob_more_than_11\n#&gt;               &lt;dbl&gt;\n#&gt; 1             0.130\n\n# What is the probability that we observe a sample mean less than or equal to 9.5?\nmeans_df |&gt;\n  mutate(less_9.5 = if_else(means &lt;= 9.5,\n                            true = 1, false = 0)) |&gt;\n  summarise(prob_less_9.5 = mean(less_9.5))\n#&gt; # A tibble: 1 × 1\n#&gt;   prob_less_9.5\n#&gt;           &lt;dbl&gt;\n#&gt; 1         0.282\n\nExercise 1. Repeat the construction of the sampling distribution of the sample mean several times. How do the results change (or not)?\nExercise 2. Use the result from Stat 325 to report the theoretical distribution of the sample mean. Use this result to find \\(P(\\bar{Y}\\leq 9.5)\\) and \\(P(\\bar{Y}&gt;11)\\) analytically.\nExercise 3. What can we conclude about the sampling distribution of \\(\\bar{y}\\) when taking samples of \\(n = 5\\) from this population? How do the simulation results compare to the result from Stat 325? Use the plot below in your answer to this question.\n\ntheoretical_df &lt;- tibble(xvals = seq(mu - 4 * sigma / sqrt(n),\n                                     mu + 4 * sigma / sqrt(n),\n                                     length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma / sqrt(n)))\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20,\n                 aes(y = after_stat(density))) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n)) +\n  geom_line(data = theoretical_df, aes(x = xvals, y = xvals_density))\n\n\n\n\n\n\n\nExercise 4. Increase the sample size to 50 and then reconstruct the sampling distribution of the sample mean for \\(n = 50\\). How do the results (mean, standard deviation, and probabilities) change? Do the changes make sense and do they match the theoretical result from Stat 325?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#lab-1.2-samp.-dist.-with-a-non-normal-population",
    "href": "samp-dist.html#lab-1.2-samp.-dist.-with-a-non-normal-population",
    "title": "1  Sampling Distributions",
    "section": "Lab 1.2: Samp. Dist. with a Non-Normal Population",
    "text": "Lab 1.2: Samp. Dist. with a Non-Normal Population\nNow consider an Exponential(\\(\\lambda\\) = 0.5) population. Go back to a sample size of \\(n = 5\\) and continue calculating the sample mean, \\(\\bar{y}\\). Before beginning, you should make sure to load in the tidyverse library again so that we can make some plots:\n\nlibrary(tidyverse)\n\nThe code below modifies the code from the previous section on a normal population model to reflect the updated exponential population model. Copy the code and run it in your own R session to obtain the graph of the exponential population model with a random sample of \\(n = 5\\) observations.\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n, lambda) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n# compute the sample mean\nsample_mean &lt;- mean(single_sample)\n# look at the sample mean\nsample_mean \n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dexp(xvals, lambda))\n\n## plot the population model density curve\nggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  ## add the sample points from your sample\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  ## add a line for the sample mean\n  geom_vline(xintercept = sample_mean, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Exponential with Lambda = 0.5\")\n\nNow that we have an idea of what the population model looks like and we can generate a single sample from this model (along with the sample mean), we can repeat the generation of the sample mean thousands of times to construct the sampling distribution of the sample mean when \\(n = 5\\) for the exponential model.\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_exp_mean &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_mean &lt;- mean(single_sample)\n  \n  return(sample_mean)\n}\n\n## test function once:\ngenerate_exp_mean(lambda = lambda, n = n)\n#&gt; [1] 5.047044\n\nnsim &lt;- 5000      # number of simulations\n\nmeans &lt;- map_dbl(1:nsim, \\(i) generate_exp_mean(lambda = lambda, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmeans_df &lt;- tibble(means)\nmeans_df\n#&gt; # A tibble: 5,000 × 1\n#&gt;   means\n#&gt;   &lt;dbl&gt;\n#&gt; 1  2.23\n#&gt; 2  1.04\n#&gt; 3  1.67\n#&gt; 4  1.64\n#&gt; 5  1.31\n#&gt; 6  1.11\n#&gt; # ℹ 4,994 more rows\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"darkolivegreen4\", fill = \"darkolivegreen1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nExercise 1. In the code that generated the graph for the population model of the exponential distribution, along with the single sample and its mean, I modified the code from the normal distribution population model to examine a single sample from the known population, but for the Exponential population. What are some changes I made and why?\nExercise 2. Now look at the sampling distribution of the sample mean when \\(n = 5\\) for the exponential population model. Summarise what you notice about the sampling distribution of \\(\\bar{y}\\) when taking a sample of size \\(n = 5\\) from an Exponential(\\(\\lambda\\) = 0.5) population.\nExercise 3. Increase the sample size to \\(n = 50\\). What do you notice about the sampling distribution of \\(\\bar{y}\\) now? Why has the shape of the sampling distribution changed?\nExercise 4. In general, what are some other ways you could summarise a sample of data? (i.e., other calculations you could do?)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#lab-1.3-samp.-dist.-of-the-sample-minimum",
    "href": "samp-dist.html#lab-1.3-samp.-dist.-of-the-sample-minimum",
    "title": "1  Sampling Distributions",
    "section": "Lab 1.3: Samp. Dist. of the Sample Minimum",
    "text": "Lab 1.3: Samp. Dist. of the Sample Minimum\nAgain, load in the tidyverse library so that we can make some plots!\n\nlibrary(tidyverse)\n\nLet’s return to our original Normal(\\(\\mu\\) = 10, \\(\\sigma^2\\) = 4), but let’s consider a different statistic: the sample minimum. Examine the code that you ran in an earlier lab below.\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 2   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n# compute the sample mean\nsample_mean &lt;- mean(single_sample)\n# look at the sample mean\nsample_mean   \n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  ## add the sample points from your sample\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  ## add a line for the sample mean\n  geom_vline(xintercept = sample_mean, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 2\")\n\nExercise 1. Modify the code so that, instead of calculating the sample mean as the sample statistic, you calculate the sample minimum as the sample statistic. Then, re-run the code so that you better understand what the sampling distribution of the sample minimum might look like.\nExercise 2. Predict what might happen next! That is, based on what you’ve seen by re-running the code above, where do you expect the center of the sampling distribution of the sample minimum to be relative to 10? Do you expect the sampling distribution of the sample minimum to overlap with the value 10 at all (if \\(n = 5\\)).\nExercise 3. Modify the code below so that you generate the sampling distribution of the sample minimum instead of the sample mean.\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_mean &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_mean &lt;- mean(single_sample)\n  \n  return(sample_mean)\n}\n\n## test function once:\ngenerate_samp_mean(mu = mu, sigma = sigma, n = n)\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmeans &lt;- map_dbl(1:nsim, \\(i) generate_samp_mean(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmeans_df &lt;- tibble(means)\nmeans_df\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\nmeans_df |&gt;\n  summarise(mean_samp_dist = mean(means),\n            var_samp_dist = var(means),\n            sd_samp_dist = sd(means))\n\nmeans_df |&gt;\n  mutate(more_than_11 = if_else(means &gt; 11,\n                                true = 1, false = 0)) |&gt;\n  summarise(prob_more_than_11 = mean(more_than_11))\n\nmeans_df |&gt;\n  mutate(less_9.5 = if_else(means &lt;= 9.5,\n                            true = 1, false = 0)) |&gt;\n  summarise(prob_less_9.5 = mean(less_9.5))\n\nExercise 4. Summarise what you notice about the sampling distribution. How does it compare to the sampling distribution of the sample mean? Does that make sense? Why or why not?\nExercise 5. Report the probability that the sample minimum is less than or equal to 9.5. How does it compare to the probability that the sample mean is less than or equal to 9.5? Does that make sense? Why or why not?\nExercise 6. Increase the sample size to 50. Summarize how the sampling distribution of the sample minimum when \\(n = 50\\) differs from when \\(n = 5\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#mini-project-1-sampling-distribution-of-the-sample-minimum-and-maximum",
    "href": "samp-dist.html#mini-project-1-sampling-distribution-of-the-sample-minimum-and-maximum",
    "title": "1  Sampling Distributions",
    "section": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "text": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum\nOn our second day of class, we conducted a simulation to investigate the sampling distribution of the sample minimum (\\(Y_{min}\\)) when taking samples of \\(n = 5\\) observations from a Normal(\\(\\mu = 10, \\sigma^2 = 4\\)) population. For your recap of that day, you investigated the sampling distribution of the sample maximum (\\(Y_{max}\\)) from the same population (using the same sample size).\nMany of you noticed that, in this situation, SE(\\(Y_{min} \\approx\\) SE(\\(Y_{max}\\)), and many of you provided great explanations of why you thought that was true. The purpose of this mini-project assignment is for you to investigate this phenomenon to see if it is a result that holds more generally.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "2  Estimation",
    "section": "",
    "text": "Lab 2.1: Maximum Likelihood Estimation\nIn this subsection, we make a few plots of various likelihoods we have encountered so far. Note that the likelihood plot will change for different data, and, if you would like, you can put in different data vectors to see how the likelihood changes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#lab-2.1-maximum-likelihood-estimation",
    "href": "estimation.html#lab-2.1-maximum-likelihood-estimation",
    "title": "2  Estimation",
    "section": "",
    "text": "MLE Example: Binomial Likelihood\nThe code below examines the likelihood of \\(p\\) in a binomial setting with known \\(n\\). The peak shown in the plot gives the value of \\(p\\) that maximizes this likelihood.\n\nlibrary(tidyverse)\n\n## get binomial likelihood for each value of p\n## p is a vector of probabilities\n## n is a single value for the sample size\n## dat is a vector of data\n\nget_binom_lik &lt;- function(p, n, data_vec) {\n  \n  ## for each value of p, map through the dbinom function for\n  ## each data point (and multiply the results at the end to obtain\n  ## the likelihood)\n  binom_lik &lt;- map_dbl(p,\n                       ~ dbinom(x = data_vec, size = n, prob = .x) |&gt;\n                         prod() ## like the capital Pi in formula\n  )\n  \n  ## the function returns a vector of likelihoods for each\n  ## candidate p (and will be a vector of the same length as p)\n  return(binom_lik)\n}\n\n\np &lt;- seq(0, 1, by = 0.001)\nn &lt;- 20\ndat &lt;- c(5, 4, 2, 1, 0, 1, 2)\n\nbinom_liks &lt;- get_binom_lik(p = p, n = n, data_vec = dat)\n\nlik_df &lt;- tibble(p, binom_liks)\nggplot(data = lik_df, aes(x = p, y = binom_liks)) +\n  geom_line() +\n  labs(y = \"Likelihood\") +\n  theme_minimal()\n\nMLE Example\nIn class, we derived the MLE for the parameter \\(\\theta\\) in the probability model:\n\\[\nf(x \\vert \\theta) = \\frac{2x}{\\theta} e^{-\\frac{x^2}{\\theta}},\n\\]\nHere, we graph that likelihood for a sample of \\(n = 4\\) data points.\n\nthetas &lt;- seq(0, 30, length.out = 1000)  ## may need to adjust these limits\ndat &lt;- c(3, 1, 2, 4)\n\nget_theta_lik &lt;- function(theta, data_vec) {\n  ## since there is no \"dbinom\" equivalent for the unnamed probability model\n  ## we have to code f(x \\vert \\theta) \"by hand\":\n  theta_lik &lt;- map_dbl(theta,\n                       ~ ((2 * data_vec / .x) * exp(-data_vec^2 / .x)) |&gt;\n                         prod()\n  )\n  \n  return(theta_lik)\n}\n \ntheta_liks &lt;- get_theta_lik(theta = thetas, data_vec = dat)\n\nlik_df &lt;- tibble(thetas, theta_liks)\nggplot(data = lik_df, aes(x = thetas, y = theta_liks)) +\n  geom_line() +\n  labs(y = \"Likelihood\") +\n  theme_minimal()\n\nMLE Example: Poisson\nFinally, consider the likelihood of various values of \\(\\lambda\\), where the random sample of data is from a Poisson probability model.\n\ndat &lt;- c(8, 0, 4, 9, 1)\nlambdas &lt;- seq(0, 10, length.out = 1000) ## may need to adjust these limits\n\n\nget_poisson_lik &lt;- function(lambda, data_vec) {\n  \n  poisson_lik &lt;- map_dbl(lambda,\n                         ~ dpois(x = data_vec, lambda = .x) |&gt;\n                           prod()\n  )\n  \n  return(poisson_lik)\n}\n\npoisson_liks &lt;- get_poisson_lik(lambda = lambdas, data_vec = dat)\nlik_df &lt;- tibble(lambdas, poisson_liks)\nggplot(data = lik_df, aes(x = lambdas, y = poisson_liks)) +\n  geom_line() +\n  labs(y = \"Likelihood\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#lab-2.2-consistency",
    "href": "estimation.html#lab-2.2-consistency",
    "title": "2  Estimation",
    "section": "Lab 2.2: Consistency",
    "text": "Lab 2.2: Consistency\nIn this subsection, we illustrate a couple of consistent estimators.\nUniform Example\n\ntheta &lt;- 10  ## specify an upper bound for our uniform model\n## recall that the lower bound is known and is equal to 0\n\nnsim &lt;- 1000 ## specify range of possible sample sizes (how long we will run the simulation)\n\n## compute each estimator of theta using adjusted MLE, MLE, and MOM\ncompute_unif_estimators &lt;- function(theta, nsim) {\n  \n  unif_draws &lt;- runif(n = nsim, min = 0, max = theta)\n\n  ## compute the method of moment estimator \n  ## nsim different times (once using a sample of size 1, then\n  ## using a sample of size 2, ...., then using a sample of size nsim)\n  unif_mom &lt;- map_dbl(1:nsim, ~ 2 * mean(unif_draws[1:.x]))\n  \n  ## same type of computation for MLE\n  unif_mle &lt;- map_dbl(1:nsim, ~ max(unif_draws[1:.x]))\n  \n  ## same type of computation for adjusted MLE\n  unif_mle_adj &lt;- map_dbl(1:nsim,\n                          ~ ((.x + 1) / .x) * max(unif_draws[1:.x]))\n\n  ## put all three results into a data frame, along with n\n  output_df &lt;- tibble(n = 1:nsim, \n         unif_mom, unif_mle, unif_mle_adj)\n  \n  return(output_df)\n}\n\nplot_df &lt;- compute_unif_estimators(theta = theta, nsim = nsim)\n\n\n## do some DATA/STAT 234-type Work to make the plot\n\nplot_long &lt;- plot_df |&gt; pivot_longer(cols = starts_with(\"unif\"),\n                                     names_to = \"Estimator\",\n                                     values_to = \"theta_estimate\")\nggplot(data = plot_long, aes(x = n,\n                             y = theta_estimate,\n                             colour = Estimator)) +\n  geom_line() +\n  geom_hline(yintercept = theta, linetype = 2) +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"n (Sample Size)\",\n       caption = \"Dotted Line Shows True Value of Theta\")\n\nBinomial Example\n\np &lt;- 1 / 4 ## specify probability of success for the simulation\n\nnsim &lt;- 1000 ## specify range of possible ns (how long we will run the simulation)\n\ncompute_binom_estimator &lt;- function(p, nsim) {\n  \n  ## draw nsim different of 0's and 1's\n  bernoulli_draws &lt;- rbinom(n = nsim, size = 1, prob = p)\n\n  ## compute the estimator of p\n  ## nsim different times (once using a sample of 1 bernoulli, then\n  ## using a sample of 2 bernoullis, ....,\n  ##  then using a sample of nsim bernoullis)\n  \n  binom_mle &lt;- map_dbl(1:nsim, ~ sum(bernoulli_draws[1:.x]) / .x)\n\n\n  ## put all three results into a data frame, along with n\n  output_df &lt;- tibble(n = 1:nsim, binom_mle)\n  \n  return(output_df)\n}\n\nplot_df &lt;- compute_binom_estimator(p = p, nsim = nsim)\n\nggplot(data = plot_df, aes(x = n,\n                             y = binom_mle)) +\n  geom_line() +\n  geom_hline(yintercept = p, linetype = 2) +\n  theme_minimal() +\n  labs(x = \"n (Sample Size)\",\n       caption = \"Dotted Line Shows True Value of p\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#mini-project-2-a-meaningful-story",
    "href": "estimation.html#mini-project-2-a-meaningful-story",
    "title": "2  Estimation",
    "section": "Mini Project 2: A Meaningful Story",
    "text": "Mini Project 2: A Meaningful Story\nPut description of project here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "conf-int.html",
    "href": "conf-int.html",
    "title": "3  Confidence Intervals",
    "section": "",
    "text": "Lab 3.1: Introduction to Plotting and Data Summarisation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#lab-3.1-introduction-to-plotting-and-data-summarisation",
    "href": "conf-int.html#lab-3.1-introduction-to-plotting-and-data-summarisation",
    "title": "3  Confidence Intervals",
    "section": "",
    "text": "Plotting\nIf we are not using the Central Limit Theorem to obtain the (approximate) sampling distribution of the sample mean, we must plot the data to determine if it could have plausibly come from a Normal population (looking for rough symmetric and absence of outliers). While we have lots of options here (including histograms and normal quantile plots), I like a boxplot, especially in two sample situations.\nFor this illustration, we’ll use the Beerwings dataset from the textbook to show how to make plots of a single quantitative variable.\n\nlibrary(tidyverse)\nlibrary(resampledata)\n\n## convert Beerwings data frame to a tibble for nicer printing\nbeerwings_df &lt;- Beerwings |&gt; as_tibble() \nbeerwings_df\n\nLet’s first consider the data as a single sample and explore the distributions of both Beer consumption and Hotwing consumption:\n\nggplot(data = beerwings_df, aes(x = Beer)) +\n  geom_histogram(bins = 8, colour = \"black\", fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(x = Beer)) +\n  geom_boxplot(fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(sample = Beer)) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_minimal()\n\n\nggplot(data = beerwings_df, aes(x = Hotwings)) +\n  geom_histogram(bins = 8, colour = \"black\", fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(x = Hotwings)) +\n  geom_boxplot(fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(sample = Hotwings)) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_minimal()\n\nNow, let’s compare the distribution of hotwings for each sex in the data set with a set of side-by-side boxplots:\n\nggplot(data = beerwings_df, aes(x = Gender, y = Hotwings)) +\n  geom_boxplot(fill = \"steelblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\nData Summarisation and Creation\nNow, let’s examine how we can look at some summary statistics of the Hotwings variable in the beerwings_df data. For one sample, we can just use the summarise() function with whatever summary metrics we want to compute:\n\nbeerwings_df |&gt;\n  summarise(xbar = mean(Hotwings),\n            sd = sd(Hotwings),\n            n = n())\n\nIf we want to compare summary statistics for each level of a categorical variable, we can first group_by() that variable:\n\n# Two samples\nbeerwings_df |&gt;\n  group_by(Gender) |&gt;\n  summarise(xbar = mean(Hotwings),\n            sd = sd(Hotwings),\n            n = n())\n\nFinally, though this is not a data summary function, we can use filter() if we want to keep only certain rows in the data set. For example, if we want to get rid of any people who did not consume any Beer, we can use:\n\nbeerwings_onlybeer = beerwings_df |&gt; filter(Beer &gt; 0)\n\nCreating a Data Set with tibble()\n\nSuppose we need to enter your own data into R. As long as the sample size is relatively small (small enough that you can type things in manually without it being too much of a pain), we can create a data frame object using the tibble() function. For example, on the Stat 113 First Day survey, we ask students how long (in hours) it takes them to travel from home to SLU. Here are 6 responses: 6, 3.5, 0.4, 2.5, 12, 6.\nWe can make a data frame with those values:\n\nstat113_df &lt;- tibble(Travel = c(6, 3.5, 0.4, 2.5, 12, 6))\nstat113_df\n\n# A tibble: 6 × 1\n  Travel\n   &lt;dbl&gt;\n1    6  \n2    3.5\n3    0.4\n4    2.5\n5   12  \n6    6  \n\n\nIn the code, stat113_df is the name of the data frame while Travel is the name of the variable within that data frame.\nInstead of entering in raw data, we can also simulate data and put that simulated data into a data frame. For example, let’s convince ourselves that small samples from Normal populations don’t always look “Normal” by simulating a small sample of normally distributed data:\n\nfakedata &lt;- tibble(Normals = rnorm(n = 14, 15, 4))\n\nggplot(data = fakedata, aes(x = Normals)) + \n  geom_boxplot(fill = \"coral\") +\n  theme_minimal()\n\n\n\n\n\n\nggplot(data = fakedata, aes(x = Normals)) + \n  geom_histogram(fill = \"coral\", colour = \"coral4\", bins = 10) +\n  theme_minimal()\n\n\n\n\n\n\n\nRerun the code above a few times. What do you notice about the boxplots and histograms that you are generating?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#lab-3.2-conceptual-confidence-intervals",
    "href": "conf-int.html#lab-3.2-conceptual-confidence-intervals",
    "title": "3  Confidence Intervals",
    "section": "Lab 3.2: Conceptual Confidence Intervals",
    "text": "Lab 3.2: Conceptual Confidence Intervals\nThe primary purpose of this lab is to explore what the concept of “confidence” really means. To do so, we will construct confidence intervals first for a population mean (\\(\\mu\\)) using the “one-sample t” confidence interval formula .\nOne Sample t (CI for \\(\\mu\\))\nIf we were to repeatedly generate random samples from a population and use those samples to construct (1 - \\(\\alpha\\))% confidence intervals, the coverage rate is defined as the fraction of those intervals that actually contain (capture) the parameter.\nIn the code below, we explore the average width and the coverage rate of confidence intervals for \\(\\mu\\) using the “standard” one-sample t formula.\n\ngenerate_onesamp_cis &lt;- function(n, mu, sigma, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x &lt;- rnorm(n, mu, sigma)\n  \n  ## compute the bounds of the ci\n  point_est &lt;- mean(x)\n  lb &lt;- point_est - qt(1 - alpha/2, df = n - 1) * sd(x) / sqrt(n)\n  ub &lt;- point_est + qt(1 - alpha/2, df = n - 1) * sd(x) / sqrt(n)\n  \n  ## put everything into a tibble\n  out_df &lt;- tibble(point_est, lb, ub)\n  \n  return(out_df)\n}\n\n\n## define parameters to use in our function\nn &lt;- 5   # sample size\nmu &lt;- 10     # true mean\nsigma &lt;- 5    # true standard deviation\nalpha &lt;- 0.05  # used to construct 1-alpha CI (how much area should be in the tails)\n\n## generate one sample and one ci\ngenerate_onesamp_cis(n = n, mu = mu, sigma = sigma, alpha = alpha)\n\nBut, if we want to explore the coverage rate and average interval width, we need to simulate many confidence intervals. Below, we map() through our function nsim times and bind the results together into a data frame at the end:\n\nnsim &lt;- 1000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, mu = mu,\n                                            sigma = sigma,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 1,000 × 3\n   point_est    lb    ub\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     11.1  5.51   16.7\n 2     10.5  6.14   14.8\n 3      9.31 1.30   17.3\n 4      9.12 1.35   16.9\n 5      9.60 3.51   15.7\n 6      8.57 0.191  16.9\n 7      8.82 1.73   15.9\n 8     11.1  7.02   15.2\n 9     13.1  8.05   18.1\n10     12.0  2.31   21.7\n# ℹ 990 more rows\n\n\nFinally, since we are interested in average interval width and coverage rate, we can create variables for the width of each confidence interval and for whether or not each interval “covers” the true mean mu:\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(mu &gt; lb & mu &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nAnd we can then summarise() to get the average width and the coverage rate:\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1      11.7         0.949\n\n\nExercise.\nNote that if you wanted to compare the performance of the one-sample t formula using Normal data and non-Normal data, it is important that you generate data from those populations in such a way that the populations have the same means and standard deviations (otherwise the comparisons would not be fair - especially any comparisons of average width!). Here are some guidelines about how you might do that.\nAn Exponential Population: We know for an Exponential random variable \\(Y\\) that \\(E(Y) = 1 / \\lambda\\) and \\(Var(Y) = 1 / \\lambda^2 = (1 / \\lambda)^2\\). We notice here, that for Exponential random variables, the variance is the mean squared (which implies that the standard deviation equals the mean). Thus, if you wanted to compare the performance of the one-sample t formula for Normal and Exponential data, you would need to focus exclusively on situations where \\(mu = sigma\\) and you could identify which value of \\(lambda\\) is necessary to achieve your desired mean.\nChi-square would be another distribution where there is a pretty easy relationship between mean and standard deviation (and therefore choose the df for Chi-Square). Gamma is also reasonble, but does require a little bit of algebra to identify the \\(\\alpha\\) and \\(\\lambda\\) needed to get the desired mean and standard deviation.\nTwo Sample t\nNext, we suppose that we have two populations and that we are interested in constructing a confidence interval for the difference in population means, \\(\\mu_1 - mu_2\\). To do so, we will not assume that the underyling variances of each population are equal and will instead use The Welch-Satterthwaite approximation to degrees of freedom for the \\(t^*\\) value to create the confidence interval.\n\nn1 &lt;- 15         # size of first sample\nmu1 &lt;- 20        # mean of first population\nsigma1 &lt;- 5      # standard deviation of first population\n\nn2 &lt;- 15         # size of second sample\nmu2 &lt;- 15        # mean of second population\nsigma2 &lt;- 5      # standard deviation of second population\n\nx &lt;- rnorm(n1, mu1, sigma1)       # generate sample 1 from Population 1\ny &lt;- rnorm(n2, mu2, sigma2)       # generate sample 2 from Population 2\n\nvarest1 &lt;- var(x)\nvarest2 &lt;- var(y)\n\nA &lt;- varest1 / n1\nB &lt;- varest2 / n2\n\nnumerator &lt;- (A + B) ^2\ndenominator &lt;- A^2 / (n1 - 1) + B^2 / (n2 - 1)\n\nwelchdf &lt;- numerator / denominator\nwelchdf\n\nGenerating Sample Proportions\nFinally, if we are interested in constructing a confidence interval for a population proportion instead of a population mean (so, our variable of interest is categorical with two levels), then we can use the following code to generate a sample proportion from a population with a true proportion of success \\(p\\) and a certain sample size \\(n\\):\n\nn &lt;- 10   # sample size\np &lt;- 0.5  # population proportion\n  \nx &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n## number of successes divided by sample size\nphat &lt;- x / n\nphat",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#lab-3.3-bootstrap-confidence-intervals",
    "href": "conf-int.html#lab-3.3-bootstrap-confidence-intervals",
    "title": "3  Confidence Intervals",
    "section": "Lab 3.3: Bootstrap Confidence Intervals",
    "text": "Lab 3.3: Bootstrap Confidence Intervals\nThe purpose of this lab is to construct bootstrap confidence intervals for a population mean, a population proportion, and for a difference in population means. To start, we construct a bootstrap confidence interval for a population mean.\nBootstrap CI for a Population Mean\nBelow, the number of hours of exercise per week for a random sample of Stat 113 students are provided.\n\nlibrary(tidyverse)\nstat113_ex &lt;- tibble(Exercise = c(12, 3, 4, 10, 8, 17, 15, 5, 8, 10, 8,\n                                  25, 1, 15, 2, 14, 10, 8, 6, 14, 5, 6,\n                                  12, 3))\n\n# for demonstrating danger of bootstrapping with small sample sizes\n#stat113_ex = tibble(Exercise = c(25, 1, 2, 4, 0, 3, 0))\n\nBefore constructing any confidence interval, we shoud create a plot of the data and obtain the sample mean:\n\nggplot(data = stat113_ex, aes(x = Exercise)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 10) +\n  theme_minimal()\n\n\n\n\n\n\nmean(stat113_ex$Exercise) # sample mean\n\n[1] 9.208333\n\n\nNext, we create the bootstrap distribution of sample means and make a histogram of that distribution. For this example, we generate 1000 bootstrap samples. The function below outputs one bootstrap mean from a single bootstrap sample:\n\ngenerate_one_bootmean &lt;- function() {\n  ## put exercise variable into a vector\n  exercise_vec &lt;- stat113_ex |&gt; pull(Exercise)\n  \n  ## sample the exercise data with replacement\n  boot_samp &lt;- sample(exercise_vec, size = length(exercise_vec),\n                      replace = TRUE)\n  \n  boot_mean &lt;- mean(boot_samp)\n  \n  return(boot_mean)\n}\n\ngenerate_one_bootmean()\n\nWe can then map over the function to obtain B bootstrap means:\n\nB &lt;- 2000\nbootmean_vec &lt;- map_dbl(1:B, \\(i) generate_one_bootmean())\n\nWith our 2000 bootstrap means, we can construct a plot of the bootstrap distribution. This distribution should be somewhat centered at the sample mean.\n\nbootmean_df &lt;- tibble(bootmean_vec)\nggplot(data = bootmean_df, aes(x = bootmean_vec)) +\n  geom_histogram(colour = \"black\", fill = \"mediumseagreen\", bins = 14) +\n  geom_vline(xintercept = mean(stat113_ex$Exercise),\n             colour = \"grey\", linewidth = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe can also compute some quantities of interest that we discussed in class:\n\n## mean of bootstrap distribution (should be close to our statistic)\nmean(bootmean_vec)\n\n## Bootstrap SE\nsd(bootmean_vec)\n\n# bootstrap estimate of bias\nbias &lt;- mean(bootmean_vec) - mean(stat113_ex$Exercise)\nbias\n\n# what percent of the BS variation is due to bias?\nabs(bias) / sd(bootmean_vec)\n\n## Percentile Bootstrap confidence intervals\n# 90% CI\nquantile(bootmean_vec, c(0.05, 0.95))\n# 95% CI\nquantile(bootmean_vec, c(0.025, 0.975))\n\nBootstrap CI for a Population Proportion\nIn a random sample of 588 adults, 16.67% reported that key lime pie is their favorite kind of pie (only 16.67%….key lime is the GOAT of pies). Construct a 95% Bootstrap percentile confidence interval for the proportion of all adults that have key lime as their favorite pie.\n\n## create a data frame where a 1 corresponds to a person who\n## chose key lime pie while a 0 corresponds to a person who\n## chose anything else other than key lime pie\n\nn &lt;- 588\np_hat &lt;- 0.1667\npie_df &lt;- tibble(key_lime_fav = c(rep(1, round(n * p_hat)),\n                                  rep(0, n - round(n * p_hat))))\n\n## sample proportion (double check)\nmean(pie_df$key_lime_fav)\n\n[1] 0.1666667\n\n\nFirst, we write a function to obtain a single bootstrap proportion from a single bootstrap sample:\n\ngenerate_one_bootprop &lt;- function() {\n  ## put exercise variable into a vector\n  pie_vec &lt;- pie_df |&gt; pull(key_lime_fav)\n  \n  ## sample the exercise data with replacement\n  boot_samp &lt;- sample(pie_vec, size = length(pie_vec),\n                      replace = TRUE)\n  \n  boot_prop &lt;- mean(boot_samp)\n  \n  return(boot_prop)\n}\n\ngenerate_one_bootprop()\n\n[1] 0.1734694\n\n\nWe can then map over the function to obtain B bootstrap proportions and put these bootstrap proportions into a data frame:\n\nB &lt;- 5000\nbootprop_vec &lt;- map_dbl(1:B, \\(i) generate_one_bootprop())\n\nbootprop_df &lt;- tibble(bootprop_vec)\n\nExercise. Construct a histogram of the bootstrap sample proportions, adding in a vertical line for the sample proportion.\nExercise. Compute quantities of interest to help construct a percentile confidence interval for the true proportion. You should compute: the center of the bootstrap distribution, the bootstrap standard error, the bootstrap estimate of bias, the percent of the boostrap variation that is due to bias, a 90% percentile-based confidence interval for the true proportion, and a 95% percentile-based confidence interval for the true proportion.\nExercise. Interpret the 90% confidence interval in context of the problem.\nBootstrap CI’s with Two (Independent) Samples\nFinally, we construct a bootstrap confidence interval using two independent random samples. We are interested in two different quantities here: the difference in population means and the ratio of population means. To do so, we use the flight data, which has information on flight delays from a sample of United Airlines flights (UA) and a sample of Amaerican Airlines flights (AA). We are interested in if either of these airlines has a higher average delay than the other.\nFirst, we obtain some summary statistics and make a plot of the data:\n\nlibrary(resampledata)\ndelay_df &lt;- FlightDelays |&gt; as_tibble()\n## set.seed(13617)  # 50100 for bias illustration\n\ndelay_df |&gt; group_by(Carrier) |&gt;\n  summarise(n = n(),\n            xbar = mean(Delay),\n            sd = sd(Delay))\n\n# A tibble: 2 × 4\n  Carrier     n  xbar    sd\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 AA       2906  10.1  40.1\n2 UA       1123  16.0  45.1\n\nggplot(delay_df,\n       aes(x = Carrier, y = Delay)) + \n  geom_boxplot(fill = \"steelblue\")\n\n\n\n\n\n\n\nOur sample statistic for the difference in means and for the ratio of means can be calculated with:\n\ndelays_aa &lt;- delay_df |&gt; filter(Carrier == \"AA\")\ndelays_ua &lt;- delay_df |&gt; filter(Carrier == \"UA\")\n\ndiffmeans &lt;- mean(delays_aa$Delay) - mean(delays_ua$Delay) # our sample statistic (diff. in means)\nratiomeans &lt;- mean(delays_aa$Delay) / mean(delays_ua$Delay) # our sample statistic (ratio of means)\n\nThe code below creates a function that generates either the boostrap difference in means or the bootstrap ratio of means from a single bootstrap sample:\n\ngenerate_one_boot &lt;- function(statistic = \"diff\") {\n  ## statistic can also be \"ratio\"\n  \n  ## put exercise variable into a vector\n  aa_vec &lt;- delays_aa |&gt; pull(Delay)\n  ua_vec &lt;- delays_ua |&gt; pull(Delay)\n  \n  ## sample the exercise data with replacement\n  boot_samp &lt;- sample(pie_vec, size = length(pie_vec),\n                      replace = TRUE)\n  \n  boot_prop &lt;- mean(boot_samp)\n  \n  ## FILL IN MISSING PIECES\n\n  ## fill in piece to obtain one bootstrap sample mean for aa flights\n\n  ## fill in piece to obtain one bootstrap sample mean for ua flights\n  \n  if (statistic == \"diff\") {\n    \n    ## fill in piece to compute the difference in sample means\n    \n    return(boot_diff)\n    \n  } else if (statistic == \"ratio\") {\n    \n    ## fill in piece to compute the ratio of sample means\n    \n    return(boot_ratio)\n  }\n  \n}\n\nExercise. Using this function, we can obtain the bootstrap distribution of differences and the bootstrap distribution of ratios. Use the previous code to construct the histograms of these two bootstrap distributions, overlaying the sample statistics for each onto their appropriate bootstrap distributions.\nI’ve provided code to generate the bootstrap distribution of differences in means but you will need to adjust this to also create the bootstrap distribution for the ratio.\n\nB &lt;- 5000\nbootdiff_vec &lt;- map_dbl(1:B, \\(i) generate_one_boot(statistic = \"diff\"))\n\nbootdiff_df &lt;- tibble(bootdiff_vec)\n\nExercise. Compute relevant quantities of interest to help create a 90% percentile-based bootstrap confidence interval for the true mean difference in delay times and the true ratio of delay times.\nExercise. Interpret your two bootstrap confidence intervals in context of the problem.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#mini-project-3-simulation-to-investigate-confidence-intervals",
    "href": "conf-int.html#mini-project-3-simulation-to-investigate-confidence-intervals",
    "title": "3  Confidence Intervals",
    "section": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "text": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "4  Bayesian Statistics",
    "section": "",
    "text": "Lab 4.1: Introduction to Bayesian: Binomial Data\nIn this lab, we will analyze binomial data in a Bayesian framework with a couple of different prior distributions for \\(p\\). The first prior for \\(p\\) will be a non-informative prior while the second prior for \\(p\\) will be based on “expert” (your!) opinion prior to the analysis.\nTo start, consider your own prowess at the popular game “flip cup” and your (self-assessed) probability of successfully flipping the cup so that it’s upside-down. Use the app at http://shiny.stlawu.edu:3838/sample-apps/stat325/distplot/ to drag the sliders around until you settle on a reasonable informative prior distribution for the probability that you successfully flip a cup from right-side-up to upside-down (note that you should not use a non-informative prior here). In general:\nWrite down the parameters you will use for your informative prior of you successfully flipping a cup.\nIn comparison, we will also use a Beta(1, 1) = Uniform(0, 1) prior distribution for \\(p\\). This is a non-informative prior.\nAfter you have settled on your informative prior, use the code below to construct a plot of both the informative prior and the non-informative prior.\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\n\ninformative_alpha &lt;- 2\ninformative_beta &lt;- 4\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_prior &lt;- dbeta(ps, informative_alpha,\n                           informative_beta)\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\nprior_plot &lt;- tibble(ps, informative_prior, noninformative_prior) |&gt;\n  pivot_longer(2:3, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\nExercise. With our informative prior and our non-informative prior, we will now collect some data to include in our bayesian analysis. After you collect data, derive the posterior distribution of \\(p\\) using both the noninformative prior and the informative prior. Then, adjust the code above to produce a plot of your two posterior distributions.\n## put code to make your posterior distribution plot here\nExercise. For each posterior, compute the posterior mean.\nExercise. For each posterior, compute a 95% credible interval for \\(p\\).\nExercise. The informative prior that you used was very subjective, based on your own knowledge and thoughts of how well you can play flip cup. What do you think can be done to limit the subjectivity of this prior?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayesian.html#lab-4.1-introduction-to-bayesian-binomial-data",
    "href": "bayesian.html#lab-4.1-introduction-to-bayesian-binomial-data",
    "title": "4  Bayesian Statistics",
    "section": "",
    "text": "increasing \\(\\alpha\\) will shift the distribution to the right.\nincreasing \\(\\beta\\) will shift the distribution to the left.\nincreasing both will give a distribution with less variability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayesian.html#lab-4.2-more-bayesian-poisson-data",
    "href": "bayesian.html#lab-4.2-more-bayesian-poisson-data",
    "title": "4  Bayesian Statistics",
    "section": "Lab 4.2: More Bayesian: Poisson Data",
    "text": "Lab 4.2: More Bayesian: Poisson Data\nRecall from probability that we discussed using a Poisson model to model the number of goals that the St. Lawrence women’s hockey team scores in a game. In that class, we assumed that we knew the value of \\(\\lambda\\) in that probability model and we computed a few quantities of interest under that assumption.\nUsually, however, \\(\\lambda\\) must be estimated (in a frequentist analysis) using data, or, in a Bayesian analysis, we can provide a probability model for \\(\\lambda\\) itself that is either non-informative or informed by expert opinion or prior data.\nSuppose that the women’s hockey coach says that he thinks their team scores about 3 goals per game, on average. When pressed for how “sure” they are of that answer and to give a reasonable range for what values that goal scoring rate is, they reply that they are not quite sure. But, they know that they are almost certain that the scoring rate is no less than 2 goals per game, on average.\nAs discussed in class, the conjugate prior for Poisson data is the Gamma distribution, which can be used to model \\(\\lambda\\). Using the coach’s information, come up with an informative prior for \\(\\lambda\\) with the Gamma distribution. Note that we will have some decisions to make in how to use the coach’s information to come up with an informative prior!\n\n## put work for informative prior here\n\nA relatively non-informative prior for \\(\\lambda\\) is a Gamma distribution with very small values for both \\(\\alpha\\) and \\(k\\). Note that, for the gamma distribution, I am replacing \\(\\lambda\\) with \\(k\\), so that we are not working with two different \\(\\lambda\\)’s (since the Poisson also has a parameter called \\(\\lambda\\)). Using \\(\\alpha = 0.001\\) and \\(k = 0.001\\), construct a plot of a relatively non-informative prior for \\(\\lambda\\) using the code below.\n\nalpha &lt;- 0.001\nk &lt;- 0.001\nlambda_grid &lt;- seq(0, 5, length.out = 1000)\ngamma_density &lt;- dgamma(lambda_grid, shape = alpha, rate = k)\ngamma_plot &lt;- tibble(lambda_grid, gamma_density)\nggplot(data = gamma_plot, aes(x = lambda_grid, y = gamma_density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise. Based on the plot of the non-informative prior, it’s a little difficult to see why this prior is non-informative. However, using what we derived as the posterior distribution for \\(\\lambda\\), construct an argument for why a prior of \\(\\alpha = 0.001\\) and \\(k = 0.001\\) is a non-informative prior.\nExercise. Using the code above, construct a plot of the informative prior we derived earlier.\nNow suppose that we collect data from the women’s hockey team this season to update both our non-informative prior and our informative prior with data to obtain two posterior distributions for the goal rate.\nThe code below pulls in data from the season:\n\nlibrary(rvest)\nurl &lt;- \"https://saintsathletics.com/sports/womens-ice-hockey/stats/2024-25\"\ntab &lt;- read_html(url) |&gt; \n  html_nodes(\"table\")\nhockey_stats &lt;- tab[[6]] |&gt; html_table(header = FALSE) \n\nnewnames &lt;- paste(hockey_stats[1, ], hockey_stats[2, ])\ngoals &lt;- hockey_stats |&gt; set_names(newnames) |&gt;\n  slice(-1, -2) |&gt;\n  mutate(`Shots G` = as.numeric(`Shots G`)) |&gt;\n  filter(`Shots G` &lt;= 20) |&gt; ## filter out the totals (hoping that the women\n## never scored more than 20 goals in one game!!)\n  pull(`Shots G`) |&gt; as.numeric()\ngoals\n\nExercise. Using this data and the posterior that we computed in class, figure out the posterior distribution for the goal rate with the non-informative prior and with the informative prior.\nExercise. Construct a plot of each of the posterior distributions.\nExercise. The mean of the posterior distribution must always be between the mean of the prior distribution and the mean of the data, as the posterior is a “compromise” between the prior and the observed data. Verify that this is the case for this example.\nExercise. With each posterior, compute a 95% credible interval for \\(\\lambda\\), the rate that the women’s hockey team scores goals.\nExercise. Think back to the data that we used for this example. What assumptions have we made to complete this analysis? Can you think of ways that we might relax these assumptions?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayesian.html#mini-project-4-bayesian-analysis",
    "href": "bayesian.html#mini-project-4-bayesian-analysis",
    "title": "4  Bayesian Statistics",
    "section": "Mini Project 4: Bayesian Analysis",
    "text": "Mini Project 4: Bayesian Analysis\n(likely going to use animal abundance data with three priors: a non-informative gamma prior, an informative gamma prior based on abundance survey in the previous year, and an informative prior based on wildlife manager’s knowledge of the area).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "hyp-test.html",
    "href": "hyp-test.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "Lab 5.1: Permutation Tests",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hyp-test.html#mini-project-5-advantages-and-drawbacks-of-using-p-values",
    "href": "hyp-test.html#mini-project-5-advantages-and-drawbacks-of-using-p-values",
    "title": "5  Hypothesis Testing",
    "section": "Mini Project 5: Advantages and Drawbacks of Using p-values",
    "text": "Mini Project 5: Advantages and Drawbacks of Using p-values\nSee article from The American Statistician. To make room for the bayesian project, perhaps move this to the final portfolio and cut the TAS part of the final portfolio.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "6  Portfolio",
    "section": "",
    "text": "Information about the final portfolio will be added here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portfolio</span>"
    ]
  }
]