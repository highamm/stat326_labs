[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 326 Labs and Projects",
    "section": "",
    "text": "Site Information\nWelcome! This site contains code and exercises for the labs in STAT 326 (Mathematical Statistics), information for the mini projects in the course, and information about the final portfolio. While we will also do plenty of handwritten work to help understand concepts, the primary purpose of this site is to hold all of the computing/coding work in one place.\nThe syllabus for the entire course is also given here.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#instructor-information",
    "href": "index.html#instructor-information",
    "title": "STAT 326 Labs and Projects",
    "section": "Instructor Information",
    "text": "Instructor Information\n\nProfessor: Matt Higham\nOffice: Bewkes 123\nEmail: mhigham@stlawu.edu\nOffice Hours: 15 minute slots bookable at https://calendly.com/mhigham/prof-higham-office-hours. The link to book is also on our Canvas home page.\n\nNote that you must book a time for office hours at least 12 hours in advance to guarantee that I am present and available at that time.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "STAT 326 Labs and Projects",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse Notes\n\nPrint out and bring to class.\nFolder or 3-ring binder to keep notes in.\n\nLaptop",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#general-course-outcomes",
    "href": "index.html#general-course-outcomes",
    "title": "STAT 326 Labs and Projects",
    "section": "General Course Outcomes",
    "text": "General Course Outcomes\nADD COURSE OUTCOMES",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#use-of-r-and-rstudio",
    "href": "index.html#use-of-r-and-rstudio",
    "title": "STAT 326 Labs and Projects",
    "section": "Use of R and RStudio",
    "text": "Use of R and RStudio\nWe will use RStudio throughout the semester as a tool to help us understand concepts in this course.\n\nR and RStudio are both free to use.\nPart of your first homework assignment will give some direction on how to install these on your own computer.\nThe purpose of this class is not to learn R in detail. The concepts for the course receive a strong priority.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#exams",
    "href": "index.html#exams",
    "title": "STAT 326 Labs and Projects",
    "section": "Exams",
    "text": "Exams\n\nThere will be three midterm exams, each worth 150 points. The midterm exams will all take place in the evening at 6:30 pm. The tentative dates for the three midterms are: September 25 (Wednesday), October 23 (Wednesday), and December 4 (Wednesday). More information about the midterm exams will be given closer to the date of the first exam.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#quizzes",
    "href": "index.html#quizzes",
    "title": "STAT 326 Labs and Projects",
    "section": "Quizzes",
    "text": "Quizzes\nFor most weeks during the semester, we will have an in-person quiz that covers the material from the last few days of class, given on Wednesdays. The quiz will be similar to questions from that week’s homework assignment or to questions on a recent handout. You may use a sheet that we will make together in class that contains useful R functions and some useful formalas (so please do not memorize any formulas, distributions, or R functions!!).\n\nThe first three quizzes are worth 15 points each. The remaining seven quizzes are 30 points each and, in addition to containing a question covering new material, they will also contain “cumulative questions.” The general topic of these cumulative questions will be given in advance to help you prepare.\nYou may skip one (but only one) quiz during the semester (two of the first three quizzes can count as a single “drop”) so the total number of points for the quizzes is 225 points.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#homework-assignments",
    "href": "index.html#homework-assignments",
    "title": "STAT 326 Labs and Projects",
    "section": "Homework Assignments",
    "text": "Homework Assignments\nWeekly homeworks are worth 10 points each and are due nearly every week on Wednesdays. The structure of the homework is:\n\nyou should submit a signed statement to Canvas that you have attempted each part to each homework exercise. After you submit this signed statement, a file of partial solutions will be released that you can use to check your work. Make sure to submit this so that you have enough time to grade your homework and make any necessary corrections!\nby Wednesday’s class time, you should submit a hard copy of your corrected homework with your quiz for that day. A couple of notes for this submission:\n\nyou should not erase your original work: it’s helpful for me to see this work, and it’s helpful for you to have it as well as you prepare for the exams.\nyou must make your corrections in a different colour of ink. Again, the reason for this is that it helps me see your thought process and it helps you see what you had made errors on when you go to study for the exam.\nyou will get full points for the homework if you had fully attempted each problem before checking with the solutions and that you corrected any problems for which there were mistakes.\n\n\n\n\nYou may work with other students on the homeworks, but please make sure to read the Rules for Collaboration section before doing so. If you need additional help outside of collaboration with classmate or office hours, the Peterson Quantitative Research Center (PQRC) in Valentine Hall is a great resource!\nThere will be 11 homeworks throughout the semester, and your lowest homework score will be dropped. So, the total number of points for homeworks is 100 points.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#prep-tasks",
    "href": "index.html#prep-tasks",
    "title": "STAT 326 Labs and Projects",
    "section": "Prep Tasks",
    "text": "Prep Tasks\nIn addition to homework exercises, you will also be assigned “prep tasks” to complete by the start of class every Monday. While the purpose of homework is generally to practice concepts that we have already discussed in class, the purpose of the prep tasks is generally to introduce you to a new topic and give a couple of easier practice problems with that topic so that we can spend more of class time completing more challenging problems. There will be 20 prep tasks throughout the semester, each at 5 points for a total of 100 points.\nBecause these prep tasks are meant to be low-stakes introductions to new topics, you may use more than one attempt on 3 different 5-point prep tasks throughout the semester. How you use these are entirely up to you, but the additional attempts on the prep tasks must be made prior to that task’s deadline.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#participation",
    "href": "index.html#participation",
    "title": "STAT 326 Labs and Projects",
    "section": "Participation",
    "text": "Participation\nWe will use small groups to complete handouts and practice problems in-class throughout the semester. The groups will only be used for in-class work, and we will reassign groups a few times during the semester. Your grade for participation will be based on an evaluation through a rubric that we will complete three times throughout the semester. The rubric will be given to you on the first day of class so you know right away how you will be assessed for this portion of your grade.\nAdditionally, once during the semester, you will be tasked with coming up with a “warm-up” exercise for the class to complete at the start of class. More details about this will be given later.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "STAT 326 Labs and Projects",
    "section": "Final Project",
    "text": "Final Project\nInstead of a cumulative final exam, we will be having cumulative quizzes throughout the latter two-thirds of the semester. However, we will still have a small final project. More details about the nature of this project will be provided at the end of the semester.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#point-allocation",
    "href": "index.html#point-allocation",
    "title": "STAT 326 Labs and Projects",
    "section": "Point Allocation",
    "text": "Point Allocation\nThe 1000 points possible for the class will be allocated in the following way:\n\nExams: 450 points: 150 points for each of 3 midterm exams.\nQuizzes: 225 points: 7 30-point Quizzes and 3 15-point Quizzes (with one of the 30-point quizzes or the sum of two 15-point quizzes dropped).\nHomework Assignments: 100 points: 11 Homeworks for 10 points each for a total of 100 points (with one of the 10-point homeworks dropped).\nPrep Tasks: 100 points: 20 prep tasks, each worth 5 points.\nParticipation: 60 points: 3 participation rubric evaluations at 15 points each plus one warm-up exercise and solution worth 15 points.\nFinal Project: 65 points: 1 final project.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#grading-scale",
    "href": "index.html#grading-scale",
    "title": "STAT 326 Labs and Projects",
    "section": "Grading Scale",
    "text": "Grading Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\n4.0\n3.75\n3.5\n3.25\n3.0\n2.75\n2.5\n2.25\n2.0\n1.75\n1.5\n1.25\n1.0\n0.0\n\n\n\n\nPoints\n950-1000\n920-949\n890-919\n860-889\n830-859\n810-829\n770-809\n750-769\n720-749\n700-719\n670-699\n640-669\n600-639\n0-599",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#rules-for-collaboration",
    "href": "index.html#rules-for-collaboration",
    "title": "STAT 326 Labs and Projects",
    "section": "Rules for Collaboration",
    "text": "Rules for Collaboration\nYou are allowed to collaborate with your classmates (or your classmates from the other section of this course) for homework assignments and handouts with the following rules.\n\nyou must state the name(s) of who you collaborated with at the top of each assessment.\nall work must be your own. Even if you work with someone else, you must write your answers on your own. Therefore, I expect your answers to free response questions to be at least slightly different from the person(s) you collaborated with.\nyou must not copy answers directly from the Internet or directly from the homework solutions file.\nthis isn’t a rule, but keep in mind that collaboration is not permitted on any of the exams. Therefore, when working with someone, make sure that you are both really learning so that you both can have success on the exam assessments.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#ai-policy",
    "href": "index.html#ai-policy",
    "title": "STAT 326 Labs and Projects",
    "section": "AI Policy",
    "text": "AI Policy\nThroughout the semester, we will use generative AI (specifically, ChatGPT) to aid us in performing basic calculations and in evaluating integrals. For homework assignments, the policy on AI usage will be stated at the top of the assignment. Make sure that you follow this policy. While AI can be a tool to enhance your learning, becoming overly reliant on it prevents the growth of logical thinking skills.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#diversity-statement",
    "href": "index.html#diversity-statement",
    "title": "STAT 326 Labs and Projects",
    "section": "Diversity Statement",
    "text": "Diversity Statement\nDiversity encompasses differences in age, colour, ethnicity, national origin, gender, physical or mental ability, religion, socioeconomic background, veteran status, sexual orientation, and marginalized groups. The interaction of different human characteristics brings about a positive learning environment. Diversity is both respected and valued in this classroom.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#accessibility-statement",
    "href": "index.html#accessibility-statement",
    "title": "STAT 326 Labs and Projects",
    "section": "Accessibility Statement",
    "text": "Accessibility Statement\nThe message below is copied from the Student Accessibility Services Office:\nYour experience in this class is important to me. It is the policy and practice of St. Lawrence University to create inclusive and accessible learning environments consistent with federal and state law. If you have established accommodations with the Student Accessibility Services Office in the past, please activate your accommodations so we can discuss how they will be implemented in this course.\nIf you have not yet established services through the Student Accessibility Services Office but have a temporary health condition or permanent disability that requires accommodations (conditions include but not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), please contact the Student Accessibility Services Office directly to set up a meeting. The Student Accessibility Services Office will work with you on the interactive process that establishes reasonable accommodations.\nColor Vision Deficiency: The Student Accessibility Services office can loan glasses for students who are color vision deficient. Please contact the office to make an appointment.\nFor more specific information about setting up an appointment with Student Accessibility Services please see the options listed below:\nTelephone: 315.229.5537\nEmail: studentaccessibility@stlawu.edu\nWebsite: https://www.stlawu.edu/offices/student-accessibility-services",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "STAT 326 Labs and Projects",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic dishonesty will not be tolerated. Any specific policies for this course are supplementary to the\nHonor Code. According to the St. Lawrence University Academic Honor Policy,\n\nIt is assumed that all work is done by the student unless the instructor/mentor/employer gives specific permission for collaboration.\nCheating on examinations and tests consists of knowingly giving or using or attempting to use unauthorized assistance during examinations or tests.\nDishonesty in work outside of examinations and tests consists of handing in or presenting as original work which is not original, where originality is required.\n\nClaims of ignorance and academic or personal pressure are unacceptable as excuses for academic dishonesty. Students must learn what constitutes one's own work and how the work of others must be acknowledged.\nFor more information, refer to www.stlawu.edu/acadaffairs/academic_honor_policy.pdf.\nTo avoid academic dishonesty, it is important that you follow all directions and collaboration rules and ask for clarification if you have any questions about what is acceptable for a particular assignment or exam. If I suspect academic dishonesty, a score of zero will be given for the entire assignment in which the academic dishonesty occurred for all individuals involved and Academic Honor Council will be notified. If a pattern of academic dishonesty is found to have occurred, a grade of 0.0 for the entire course can be given.\nIt is important to work in a way that maximizes your learning. Be aware that students who rely too much on others for the homework and projects tend to do poorly on the quizzes and exams.\nPlease note that in addition the above, any assignments in which your score is reduced due to academic dishonesty will not be dropped according to the homework policy e.g., if you receive a zero on a homework because of academic dishonesty, it will not be dropped from your grade.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#pqrc",
    "href": "index.html#pqrc",
    "title": "STAT 326 Labs and Projects",
    "section": "PQRC",
    "text": "PQRC\nThe Peterson Quantitative Resource Center (PQRC) offers free, no appointment necessary peer tutoring across a range of courses with quantitative content. The PQRC student staff of mentors is trained to assist students to develop and to improve their quantitative skills and understanding. More information about the PQRC’s current hours and modes of operation can be found at the PQRC webpage: https://www.stlawu.edu/offices/pqrc",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "index.html#tentative-calendar-of-due-dates",
    "href": "index.html#tentative-calendar-of-due-dates",
    "title": "STAT 326 Labs and Projects",
    "section": "Tentative Calendar of Due Dates",
    "text": "Tentative Calendar of Due Dates\nThe Exam Dates are tentatively scheduled for:\n\nExam 1: Wednesday, September 25 at 6:30 pm.\nExam 2: Wednesday, October 23 at 6:30 pm or Wednesday, October 30 at 6:30 pm.\nExam 3: Wednesday, November 20 at 6:30 pm or Wednesday, December 4 at 6:30 pm.",
    "crumbs": [
      "Site Information"
    ]
  },
  {
    "objectID": "samp-dist.html",
    "href": "samp-dist.html",
    "title": "1  Sampling Distributions",
    "section": "",
    "text": "Lab 1.1: Introduction to Statistical Simulation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#lab-1.1-introduction-to-statistical-simulation",
    "href": "samp-dist.html#lab-1.1-introduction-to-statistical-simulation",
    "title": "1  Sampling Distributions",
    "section": "",
    "text": "Starting a Simulation\nTo begin a simulation for a sampling distribution of a sample statistic, we need to choose:\n\na population model for the simulation. Let’s start with Normal(\\(\\mu\\) = 10, \\(\\sigma^2\\) = 4).\na sample size for the simulation. Let’s start with \\(n\\) = 5.\na calculation for the sample statistic that we are constructing the sampling distribution of. Let’s start with the sample mean, \\(\\bar{y}\\).\nGenerating a Single Sample Statistic\nCarefully look through this code and output to understand the process of generating a single sample from a population and computing a statistic.\nIn the code below, we simulate five observations from a normal population with mean 10 and standard deviation 2. Note that, when you run the code, you should get a different set of 5 numbers than the ones printed below: it is a random sample, after all!\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 2   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n# look at the sample\nsingle_sample \n#&gt; [1] 10.84  9.95 10.23  9.13 11.81\n\nNext, we compute the sample mean from this sample: this is our sample statistic we are interested in.\n\n# compute the sample mean\nsample_mean &lt;- mean(single_sample)\n# look at the sample mean\nsample_mean     \n#&gt; [1] 10.392\n\nAgain, your sample mean should be different!\nFinally, we can make a plot of our single sample, along with where the sample mean lies.\n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  ## add the sample points from your sample\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  ## add a line for the sample mean\n  geom_vline(xintercept = sample_mean, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 2\")\n\n\n\n\n\n\n\nConstructing the Sampling Distribution\nTo simulate the sampling distribution of the sample mean from a normal population with \\(\\mu\\) = 10 and \\(\\sigma\\) = 2 for a sample size of 5, we need to repeat the above steps many, many, many times. We can do so by\n\nWriting a function that computes the sample mean and then\nMapping through that function a large number of times and then\nPlotting the large number of sample means to examine the characteristics of the resulting distribution.\n\nFirst, let’s write the function that will compute the sample mean with a given sample size from a normal population model with a given mean and standard deviation.\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_normal_mean &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_mean &lt;- mean(single_sample)\n  \n  return(sample_mean)\n}\n\n## test function once:\ngenerate_normal_mean(mu = mu, sigma = sigma, n = n)\n#&gt; [1] 10.93026\n\nNext, to generate 5000 sample means, we map through the function:\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_normal_mean function\n## nsim times\nmeans &lt;- map_dbl(1:nsim, \\(i) generate_normal_mean(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmeans_df &lt;- tibble(means)\nmeans_df\n#&gt; # A tibble: 5,000 × 1\n#&gt;   means\n#&gt;   &lt;dbl&gt;\n#&gt; 1 10.2 \n#&gt; 2  9.20\n#&gt; 3  9.98\n#&gt; 4 12.2 \n#&gt; 5  9.79\n#&gt; 6  9.78\n#&gt; # ℹ 4,994 more rows\n\nFinally, we plot the 5000 sample means to see what our sampling distribution of the sample mean (for a sample size of 5) looks like.\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nWe can also obtain some summary statistics of the sampling distribution of the sample mean when \\(n\\) = 5:\n\nmeans_df |&gt;\n  summarise(mean_samp_dist = mean(means),\n            var_samp_dist = var(means),\n            sd_samp_dist = sd(means))\n#&gt; # A tibble: 1 × 3\n#&gt;   mean_samp_dist var_samp_dist sd_samp_dist\n#&gt;            &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1           9.99         0.779        0.883\n\nAnd, we can even obtain estimates of the probability that we observe a sample mean larger than 11 by calculating the proportion of our observed sample means that are larger than 11. We can use a similar strategy to estimate the probability that we observe a sample mean less than or equal to 9.5.\n\n# What is the probability that we observe a sample mean larger than 11?\nmeans_df |&gt;\n  mutate(more_than_11 = if_else(means &gt; 11,\n                                true = 1, false = 0)) |&gt;\n  summarise(prob_more_than_11 = mean(more_than_11))\n#&gt; # A tibble: 1 × 1\n#&gt;   prob_more_than_11\n#&gt;               &lt;dbl&gt;\n#&gt; 1             0.126\n\n# What is the probability that we observe a sample mean less than or equal to 9.5?\nmeans_df |&gt;\n  mutate(less_9.5 = if_else(means &lt;= 9.5,\n                            true = 1, false = 0)) |&gt;\n  summarise(prob_less_9.5 = mean(less_9.5))\n#&gt; # A tibble: 1 × 1\n#&gt;   prob_less_9.5\n#&gt;           &lt;dbl&gt;\n#&gt; 1         0.281\n\nExercise 1. Repeat the construction of the sampling distribution of the sample mean several times. How do the results change (or not)?\nExercise 2. Use the result from Stat 325 to report the theoretical distribution of the sample mean. Use this result to find \\(P(\\bar{Y}\\leq 9.5)\\) and \\(P(\\bar{Y}&gt;11)\\) analytically.\nExercise 3. What can we conclude about the sampling distribution of \\(\\bar{y}\\) when taking samples of \\(n = 5\\) from this population? How do the simulation results compare to the result from Stat 325? Use the plot below in your answer to this question.\n\ntheoretical_df &lt;- tibble(xvals = seq(mu - 4 * sigma / sqrt(n),\n                                     mu + 4 * sigma / sqrt(n),\n                                     length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma / sqrt(n)))\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20,\n                 aes(y = after_stat(density))) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n)) +\n  geom_line(data = theoretical_df, aes(x = xvals, y = xvals_density))\n\n\n\n\n\n\n\nExercise 4. Increase the sample size to 50 and then reconstruct the sampling distribution of the sample mean for \\(n = 50\\). How do the results (mean, standard deviation, and probabilities) change? Do the changes make sense and do they match the theoretical result from Stat 325?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#lab-1.2-samp.-dist.-with-a-non-normal-population",
    "href": "samp-dist.html#lab-1.2-samp.-dist.-with-a-non-normal-population",
    "title": "1  Sampling Distributions",
    "section": "Lab 1.2: Samp. Dist. with a Non-Normal Population",
    "text": "Lab 1.2: Samp. Dist. with a Non-Normal Population\nNow consider an Exponential(\\(\\lambda\\) = 0.5) population. Go back to a sample size of \\(n = 5\\) and continue calculating the sample mean, \\(\\bar{y}\\). Before beginning, you should make sure to load in the tidyverse library again so that we can make some plots:\n\nlibrary(tidyverse)\n\nThe code below modifies the code from the previous section on a normal population model to reflect the updated exponential population model. Copy the code and run it in your own R session to obtain the graph of the exponential population model with a random sample of \\(n = 5\\) observations.\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n, lambda) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n# compute the sample mean\nsample_mean &lt;- mean(single_sample)\n# look at the sample mean\nsample_mean \n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(0, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dexp(xvals, lambda))\n\n## plot the population model density curve\nggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  ## add the sample points from your sample\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  ## add a line for the sample mean\n  geom_vline(xintercept = sample_mean, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Exponential with Lambda = 0.5\")\n\nNow that we have an idea of what the population model looks like and we can generate a single sample from this model (along with the sample mean), we can repeat the generation of the sample mean thousands of times to construct the sampling distribution of the sample mean when \\(n = 5\\) for the exponential model.\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_exp_mean &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_mean &lt;- mean(single_sample)\n  \n  return(sample_mean)\n}\n\n## test function once:\ngenerate_exp_mean(lambda = lambda, n = n)\n#&gt; [1] 1.194018\n\nnsim &lt;- 5000      # number of simulations\n\nmeans &lt;- map_dbl(1:nsim, \\(i) generate_exp_mean(lambda = lambda, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmeans_df &lt;- tibble(means)\nmeans_df\n#&gt; # A tibble: 5,000 × 1\n#&gt;   means\n#&gt;   &lt;dbl&gt;\n#&gt; 1  1.31\n#&gt; 2  4.22\n#&gt; 3  1.92\n#&gt; 4  3.48\n#&gt; 5  2.51\n#&gt; 6  1.20\n#&gt; # ℹ 4,994 more rows\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"darkolivegreen4\", fill = \"darkolivegreen1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nExercise 1. In the code that generated the graph for the population model of the exponential distribution, along with the single sample and its mean, I modified the code from the normal distribution population model to examine a single sample from the known population, but for the Exponential population. What are some changes I made and why?\nExercise 2. Now look at the sampling distribution of the sample mean when \\(n = 5\\) for the exponential population model. Summarise what you notice about the sampling distribution of \\(\\bar{y}\\) when taking a sample of size \\(n = 5\\) from an Exponential(\\(\\lambda\\) = 0.5) population.\nExercise 3. Increase the sample size to \\(n = 50\\). What do you notice about the sampling distribution of \\(\\bar{y}\\) now? Why has the shape of the sampling distribution changed?\nExercise 4. In general, what are some other ways you could summarise a sample of data? (i.e., other calculations you could do?)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#lab-1.3-samp.-dist.-of-the-sample-minimum",
    "href": "samp-dist.html#lab-1.3-samp.-dist.-of-the-sample-minimum",
    "title": "1  Sampling Distributions",
    "section": "Lab 1.3: Samp. Dist. of the Sample Minimum",
    "text": "Lab 1.3: Samp. Dist. of the Sample Minimum\nAgain, load in the tidyverse library so that we can make some plots!\n\nlibrary(tidyverse)\n\nLet’s return to our original Normal(\\(\\mu\\) = 10, \\(\\sigma^2\\) = 4), but let’s consider a different statistic: the sample minimum. Examine the code that you ran in an earlier lab below.\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 2   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n# compute the sample mean\nsample_mean &lt;- mean(single_sample)\n# look at the sample mean\nsample_mean   \n\n# generate a range of values that span the population\nplot_df &lt;- tibble(xvals = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 500)) |&gt;\n  mutate(xvals_density = dnorm(xvals, mu, sigma))\n\n## plot the population model density curve\nggplot(data = plot_df, aes(x = xvals, y = xvals_density)) +\n  geom_line() +\n  theme_minimal() +\n  ## add the sample points from your sample\n  geom_jitter(data = tibble(single_sample), aes(x = single_sample, y = 0),\n              width = 0, height = 0.005) +\n  ## add a line for the sample mean\n  geom_vline(xintercept = sample_mean, colour = \"red\") +\n  labs(x = \"y\", y = \"density\",\n       title = \"Normal with Mu = 10 and sigma = 2\")\n\nExercise 1. Modify the code so that, instead of calculating the sample mean as the sample statistic, you calculate the sample minimum as the sample statistic. Then, re-run the code so that you better understand what the sampling distribution of the sample minimum might look like.\nExercise 2. Predict what might happen next! That is, based on what you’ve seen by re-running the code above, where do you expect the center of the sampling distribution of the sample minimum to be relative to 10? Do you expect the sampling distribution of the sample minimum to overlap with the value 10 at all (if \\(n = 5\\)).\nExercise 3. Modify the code below so that you generate the sampling distribution of the sample minimum instead of the sample mean.\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_mean &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_mean &lt;- mean(single_sample)\n  \n  return(sample_mean)\n}\n\n## test function once:\ngenerate_samp_mean(mu = mu, sigma = sigma, n = n)\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmeans &lt;- map_dbl(1:nsim, \\(i) generate_samp_mean(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmeans_df &lt;- tibble(means)\nmeans_df\n\nggplot(data = means_df, aes(x = means)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\nmeans_df |&gt;\n  summarise(mean_samp_dist = mean(means),\n            var_samp_dist = var(means),\n            sd_samp_dist = sd(means))\n\nmeans_df |&gt;\n  mutate(more_than_11 = if_else(means &gt; 11,\n                                true = 1, false = 0)) |&gt;\n  summarise(prob_more_than_11 = mean(more_than_11))\n\nmeans_df |&gt;\n  mutate(less_9.5 = if_else(means &lt;= 9.5,\n                            true = 1, false = 0)) |&gt;\n  summarise(prob_less_9.5 = mean(less_9.5))\n\nExercise 4. Summarise what you notice about the sampling distribution. How does it compare to the sampling distribution of the sample mean? Does that make sense? Why or why not?\nExercise 5. Report the probability that the sample minimum is less than or equal to 9.5. How does it compare to the probability that the sample mean is less than or equal to 9.5? Does that make sense? Why or why not?\nExercise 6. Increase the sample size to 50. Summarize how the sampling distribution of the sample minimum when \\(n = 50\\) differs from when \\(n = 5\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "samp-dist.html#mini-project-1-sampling-distribution-of-the-sample-minimum-and-maximum",
    "href": "samp-dist.html#mini-project-1-sampling-distribution-of-the-sample-minimum-and-maximum",
    "title": "1  Sampling Distributions",
    "section": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "text": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum\nAI Usage: You may not use generative AI for this project in any way.\nCollaboration: For this project, you may work with a self-contained group of 3. Keep in mind that you may not work with the same person on more than one mini-project (so, if you worked with a student on the first mini-project as a partner or in a small group, you may not work with that person on this project). Finally, if working with a partner or in a group of 3, you may submit the same code and the same table of results, but your write-up (both the short summary of your methods and your findings summary) must be written individually.\nStatement of Integrity: At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.\n“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\n\nOn our second day of class, we conducted a simulation to investigate the sampling distribution of the sample minimum (\\(Y_{min}\\)) when taking samples of \\(n = 5\\) observations from a Normal(\\(\\mu = 10, \\sigma^2 = 4\\)) population. For your recap of that day, you investigated the sampling distribution of the sample maximum (\\(Y_{max}\\)) from the same population (using the same sample size).\nMany of you noticed that, in this situation, SE(\\(Y_{min} \\approx\\) SE(\\(Y_{max}\\)), and many of you provided great explanations of why you thought that was true. The purpose of this mini-project assignment is for you to investigate this phenomenon to see if it is a result that holds more generally.\nInstructions: Use the class code as a guide to carry out simulations of the sampling distributions of the sample minimum (\\(Y_{min}\\)) and the sample maximum (\\(Y_{max}\\)) when taking samples of size \\(n = 5\\) from different populations (specified below). Fill in the summary table in this document and use it answer the questions that follow.\nSubmission: Upload your typed up solutions to the questions that appear below, graphs (population graphs, histograms of your simulated distributions of the sample minimum, and histograms of your simulated distributions of the sample maximum), and tables showing your results, and your Quarto file to Canvas.\nBelow are some things that you can use to help make your graphs and table in Quarto, should you desire to use that to render a document to submit.\n\n## create population graphs\n\nnorm_df &lt;- tibble(x = seq(3, 17, length.out = 1000),\n                  dens = dnorm(x, mean = 10, sd = 2),\n                  pop = \"normal(10, 4)\")\nunif_df &lt;- tibble(x = seq(7, 13, length.out = 1000),\n                  dens = dunif(x, 7, 13),\n                  pop = \"uniform(7, 13)\")\nexp_df &lt;- tibble(x = seq(0, 10, length.out = 1000),\n                 dens = dexp(x, 0.5),\n                 pop = \"exp(0.5)\")\nbeta_df &lt;- tibble(x = seq(0, 1, length.out = 1000),\n                  dens = dbeta(x, 8, 2),\n                  pop = \"beta(8, 2)\")\n\npop_plot &lt;- bind_rows(norm_df, unif_df, exp_df, beta_df) |&gt;\n  mutate(pop = fct_relevel(pop, c(\"normal(10, 4)\", \"uniform(7, 13)\",\n                                  \"exp(0.5)\", \"beta(8, 2)\")))\n\nggplot(data = pop_plot, aes(x = x, y = dens)) +\n  geom_line() +\n  theme_minimal() +\n  facet_wrap(~ pop, nrow = 1, scales = \"free\") +\n  labs(title = \"Population Distributions for Each Simulation Setting\")\n\n\n\n\n\n\n\nIf creating your report in Quarto, you can use a similar strategy as above to show your 4 histograms of the simulated distribution of the sample minimum and to again show your 4 histograms of the simulated distribution of the sample maximum.\nIn addition to your graphs, you should also complete the following table:\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\\(\\text{E}(Y_{min})\\)\n\n\n\n\n\n\n\\(\\text{E}(Y_{max})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n\n\n\n\n\n\n\\(\\text{SE}(Y_{max})\\)\n\n\n\n\n\n\n\nBelow is the Quarto text used to generate the table above:\n|  |  $\\text{N}(\\mu = 10, \\sigma^2 = 4)$  | $\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)$ | $\\text{Exp}(\\lambda = 0.5)$ | $\\text{Beta}(\\alpha = 8, \\beta = 2)$ |\n|:----:|:-----------------:|:-------------:|:------------:|:------------:|\n| $\\text{E}(Y_{min})$    |       |        |       |              |\n| $\\text{E}(Y_{max})$    |       |        |       |              |\n|                        |       |        |       |              |\n| $\\text{SE}(Y_{min})$   |       |        |       |              |\n| $\\text{SE}(Y_{max})$   |       |        |       |              |\n: Table of Results {.striped .hover}\nFinally, in addition to your code, graphs, and table, you should answer the following questions:\n\nBriefly discuss/describe situations when each of the above population models (Normal, Uniform, Exponential, and Beta) should be used (that is, what type of data could they model).\nBriefly summarise how \\(\\text{SE}(Y_{min})\\) and \\(\\text{SE}(Y_{max})\\) compare for each of the above population models. Can you propose a general rule or result for how \\(\\text{SE}(Y_{min})\\) and \\(\\text{SE}(Y_{max})\\) compare for a given population?\nChoose either the third (Exponential) or fourth (Beta) population model from the table above. For that population model, find the pdf of \\(Y_{min}\\)⁡ and \\(Y_{max}\\), and, for each of those random variables, sketch the pdfs and find the expected value and standard error. What do you notice about how your answers compare to the simulated answers? Some code is given below to help you plot the pdfs in R:\n\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 3, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n * exp(-(1/2) * x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "2  Estimation",
    "section": "",
    "text": "Lab 2.1: Maximum Likelihood Estimation\nIn this subsection, we make a few plots of various likelihoods we have encountered so far. Note that the likelihood plot will change for different data, and, if you would like, you can put in different data vectors to see how the likelihood changes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#lab-2.1-maximum-likelihood-estimation",
    "href": "estimation.html#lab-2.1-maximum-likelihood-estimation",
    "title": "2  Estimation",
    "section": "",
    "text": "MLE Example: Binomial Likelihood\nThe code below examines the likelihood of \\(p\\) in a binomial setting with known \\(n\\). The peak shown in the plot gives the value of \\(p\\) that maximizes this likelihood.\n\nlibrary(tidyverse)\n\n## get binomial likelihood for each value of p\n## p is a vector of probabilities\n## n is a single value for the sample size\n## dat is a vector of data\n\nget_binom_lik &lt;- function(p, n, data_vec) {\n  \n  ## for each value of p, map through the dbinom function for\n  ## each data point (and multiply the results at the end to obtain\n  ## the likelihood)\n  binom_lik &lt;- map_dbl(p,\n                       ~ dbinom(x = data_vec, size = n, prob = .x) |&gt;\n                         prod() ## like the capital Pi in formula\n  )\n  \n  ## the function returns a vector of likelihoods for each\n  ## candidate p (and will be a vector of the same length as p)\n  return(binom_lik)\n}\n\n\np &lt;- seq(0, 1, by = 0.001)\nn &lt;- 20\ndat &lt;- c(5, 4, 2, 1, 0, 1, 2)\n\nbinom_liks &lt;- get_binom_lik(p = p, n = n, data_vec = dat)\n\nlik_df &lt;- tibble(p, binom_liks)\nggplot(data = lik_df, aes(x = p, y = binom_liks)) +\n  geom_line() +\n  labs(y = \"Likelihood\") +\n  theme_minimal()\n\nMLE Example\nIn class, we derived the MLE for the parameter \\(\\theta\\) in the probability model:\n\\[\nf(x \\vert \\theta) = \\frac{2x}{\\theta} e^{-\\frac{x^2}{\\theta}},\n\\]\nHere, we graph that likelihood for a sample of \\(n = 4\\) data points.\n\nthetas &lt;- seq(0, 30, length.out = 1000)  ## may need to adjust these limits\ndat &lt;- c(3, 1, 2, 4)\n\nget_theta_lik &lt;- function(theta, data_vec) {\n  ## since there is no \"dbinom\" equivalent for the unnamed probability model\n  ## we have to code f(x \\vert \\theta) \"by hand\":\n  theta_lik &lt;- map_dbl(theta,\n                       ~ ((2 * data_vec / .x) * exp(-data_vec^2 / .x)) |&gt;\n                         prod()\n  )\n  \n  return(theta_lik)\n}\n \ntheta_liks &lt;- get_theta_lik(theta = thetas, data_vec = dat)\n\nlik_df &lt;- tibble(thetas, theta_liks)\nggplot(data = lik_df, aes(x = thetas, y = theta_liks)) +\n  geom_line() +\n  labs(y = \"Likelihood\") +\n  theme_minimal()\n\nMLE Example: Poisson\nFinally, consider the likelihood of various values of \\(\\lambda\\), where the random sample of data is from a Poisson probability model.\n\ndat &lt;- c(8, 0, 4, 9, 1)\nlambdas &lt;- seq(0, 10, length.out = 1000) ## may need to adjust these limits\n\n\nget_poisson_lik &lt;- function(lambda, data_vec) {\n  \n  poisson_lik &lt;- map_dbl(lambda,\n                         ~ dpois(x = data_vec, lambda = .x) |&gt;\n                           prod()\n  )\n  \n  return(poisson_lik)\n}\n\npoisson_liks &lt;- get_poisson_lik(lambda = lambdas, data_vec = dat)\nlik_df &lt;- tibble(lambdas, poisson_liks)\nggplot(data = lik_df, aes(x = lambdas, y = poisson_liks)) +\n  geom_line() +\n  labs(y = \"Likelihood\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#lab-2.2-consistency",
    "href": "estimation.html#lab-2.2-consistency",
    "title": "2  Estimation",
    "section": "Lab 2.2: Consistency",
    "text": "Lab 2.2: Consistency\nIn this subsection, we illustrate a couple of consistent estimators.\nUniform Example\n\ntheta &lt;- 10  ## specify an upper bound for our uniform model\n## recall that the lower bound is known and is equal to 0\n\nnsim &lt;- 1000 ## specify range of possible sample sizes (how long we will run the simulation)\n\n## compute each estimator of theta using adjusted MLE, MLE, and MOM\ncompute_unif_estimators &lt;- function(theta, nsim) {\n  \n  unif_draws &lt;- runif(n = nsim, min = 0, max = theta)\n\n  ## compute the method of moment estimator \n  ## nsim different times (once using a sample of size 1, then\n  ## using a sample of size 2, ...., then using a sample of size nsim)\n  unif_mom &lt;- map_dbl(1:nsim, ~ 2 * mean(unif_draws[1:.x]))\n  \n  ## same type of computation for MLE\n  unif_mle &lt;- map_dbl(1:nsim, ~ max(unif_draws[1:.x]))\n  \n  ## same type of computation for adjusted MLE\n  unif_mle_adj &lt;- map_dbl(1:nsim,\n                          ~ ((.x + 1) / .x) * max(unif_draws[1:.x]))\n\n  ## put all three results into a data frame, along with n\n  output_df &lt;- tibble(n = 1:nsim, \n         unif_mom, unif_mle, unif_mle_adj)\n  \n  return(output_df)\n}\n\nplot_df &lt;- compute_unif_estimators(theta = theta, nsim = nsim)\n\n\n## do some DATA/STAT 234-type Work to make the plot\n\nplot_long &lt;- plot_df |&gt; pivot_longer(cols = starts_with(\"unif\"),\n                                     names_to = \"Estimator\",\n                                     values_to = \"theta_estimate\")\nggplot(data = plot_long, aes(x = n,\n                             y = theta_estimate,\n                             colour = Estimator)) +\n  geom_line() +\n  geom_hline(yintercept = theta, linetype = 2) +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"n (Sample Size)\",\n       caption = \"Dotted Line Shows True Value of Theta\")\n\nBinomial Example\n\np &lt;- 1 / 4 ## specify probability of success for the simulation\n\nnsim &lt;- 1000 ## specify range of possible ns (how long we will run the simulation)\n\ncompute_binom_estimator &lt;- function(p, nsim) {\n  \n  ## draw nsim different of 0's and 1's\n  bernoulli_draws &lt;- rbinom(n = nsim, size = 1, prob = p)\n\n  ## compute the estimator of p\n  ## nsim different times (once using a sample of 1 bernoulli, then\n  ## using a sample of 2 bernoullis, ....,\n  ##  then using a sample of nsim bernoullis)\n  \n  binom_mle &lt;- map_dbl(1:nsim, ~ sum(bernoulli_draws[1:.x]) / .x)\n\n\n  ## put all three results into a data frame, along with n\n  output_df &lt;- tibble(n = 1:nsim, binom_mle)\n  \n  return(output_df)\n}\n\nplot_df &lt;- compute_binom_estimator(p = p, nsim = nsim)\n\nggplot(data = plot_df, aes(x = n,\n                             y = binom_mle)) +\n  geom_line() +\n  geom_hline(yintercept = p, linetype = 2) +\n  theme_minimal() +\n  labs(x = \"n (Sample Size)\",\n       caption = \"Dotted Line Shows True Value of p\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#mini-project-2-a-meaningful-story",
    "href": "estimation.html#mini-project-2-a-meaningful-story",
    "title": "2  Estimation",
    "section": "Mini Project 2: A Meaningful Story",
    "text": "Mini Project 2: A Meaningful Story\nAI Usage: You may not use generative AI for this project in any way.\nCollaboration: For this project, you may not work with other people in the class. Your story must be your own.\nStatement of Integrity: Your submission should be typed. At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.\n“All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project.”\nFor this mini-project, you will write a “meaningful story.” A “meaningful story” is one continuous piece of writing / creative work that uses key words from a list and in which the sentences “make sense and hang together.” That is, the ideas in the story must illustrate that you understand key concepts from Stat 326 in a way that allows you to write “meaningfully” about them. You may not simply write ten sequential sentences that merely define the terms; the sentences must demonstrate relationships between the terms. It is your job to use the terms in a way that demonstrates that you understand the statistical concepts involved and why we care about these terms in the big picture of statistical theory.\nIn addition, you need to frame your writing within a real-life or imaginary context or scenario. Be creative!! Write about sports or music or manufacturing cell phones or skiing trips or the zombie apocalypse. Meaningful stories could even be literary works, such as play scripts, stories, song lyrics, poetry, etc. Use your imagination when constructing your “story” and conveying the material (content MUST be appropriate).\nEstimation Prompt: Each of these terms must be incorporated into your meaningful story. Estimator, Parameter, Estimate (as a noun), Random Variable, Random Sample, Bias, Variance, Consistent, Likelihood.\nAlso: in your “meaningful story,” you must refer to at least one of our common probability distributions by name.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "conf-int.html",
    "href": "conf-int.html",
    "title": "3  Confidence Intervals",
    "section": "",
    "text": "Lab 3.1: Introduction to Plotting and Data Summarisation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#lab-3.1-introduction-to-plotting-and-data-summarisation",
    "href": "conf-int.html#lab-3.1-introduction-to-plotting-and-data-summarisation",
    "title": "3  Confidence Intervals",
    "section": "",
    "text": "Plotting\nIf we are not using the Central Limit Theorem to obtain the (approximate) sampling distribution of the sample mean, we must plot the data to determine if it could have plausibly come from a Normal population (looking for rough symmetric and absence of outliers). While we have lots of options here (including histograms and normal quantile plots), I like a boxplot, especially in two sample situations.\nFor this illustration, we’ll use the Beerwings dataset from the textbook to show how to make plots of a single quantitative variable.\n\nlibrary(tidyverse)\nlibrary(resampledata)\n\n## convert Beerwings data frame to a tibble for nicer printing\nbeerwings_df &lt;- Beerwings |&gt; as_tibble() \nbeerwings_df\n\nLet’s first consider the data as a single sample and explore the distributions of both Beer consumption and Hotwing consumption:\n\nggplot(data = beerwings_df, aes(x = Beer)) +\n  geom_histogram(bins = 8, colour = \"black\", fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(x = Beer)) +\n  geom_boxplot(fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(sample = Beer)) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_minimal()\n\n\nggplot(data = beerwings_df, aes(x = Hotwings)) +\n  geom_histogram(bins = 8, colour = \"black\", fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(x = Hotwings)) +\n  geom_boxplot(fill = \"mediumseagreen\") +\n  theme_minimal()\nggplot(data = beerwings_df, aes(sample = Hotwings)) +\n  stat_qq() +\n  stat_qq_line() +\n  theme_minimal()\n\nNow, let’s compare the distribution of hotwings for each sex in the data set with a set of side-by-side boxplots:\n\nggplot(data = beerwings_df, aes(x = Gender, y = Hotwings)) +\n  geom_boxplot(fill = \"steelblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\nData Summarisation and Creation\nNow, let’s examine how we can look at some summary statistics of the Hotwings variable in the beerwings_df data. For one sample, we can just use the summarise() function with whatever summary metrics we want to compute:\n\nbeerwings_df |&gt;\n  summarise(xbar = mean(Hotwings),\n            sd = sd(Hotwings),\n            n = n())\n\nIf we want to compare summary statistics for each level of a categorical variable, we can first group_by() that variable:\n\n# Two samples\nbeerwings_df |&gt;\n  group_by(Gender) |&gt;\n  summarise(xbar = mean(Hotwings),\n            sd = sd(Hotwings),\n            n = n())\n\nFinally, though this is not a data summary function, we can use filter() if we want to keep only certain rows in the data set. For example, if we want to get rid of any people who did not consume any Beer, we can use:\n\nbeerwings_onlybeer = beerwings_df |&gt; filter(Beer &gt; 0)\n\nCreating a Data Set with tibble()\n\nSuppose we need to enter your own data into R. As long as the sample size is relatively small (small enough that you can type things in manually without it being too much of a pain), we can create a data frame object using the tibble() function. For example, on the Stat 113 First Day survey, we ask students how long (in hours) it takes them to travel from home to SLU. Here are 6 responses: 6, 3.5, 0.4, 2.5, 12, 6.\nWe can make a data frame with those values:\n\nstat113_df &lt;- tibble(Travel = c(6, 3.5, 0.4, 2.5, 12, 6))\nstat113_df\n\n# A tibble: 6 × 1\n  Travel\n   &lt;dbl&gt;\n1    6  \n2    3.5\n3    0.4\n4    2.5\n5   12  \n6    6  \n\n\nIn the code, stat113_df is the name of the data frame while Travel is the name of the variable within that data frame.\nInstead of entering in raw data, we can also simulate data and put that simulated data into a data frame. For example, let’s convince ourselves that small samples from Normal populations don’t always look “Normal” by simulating a small sample of normally distributed data:\n\nfakedata &lt;- tibble(Normals = rnorm(n = 14, 15, 4))\n\nggplot(data = fakedata, aes(x = Normals)) + \n  geom_boxplot(fill = \"coral\") +\n  theme_minimal()\n\n\n\n\n\n\nggplot(data = fakedata, aes(x = Normals)) + \n  geom_histogram(fill = \"coral\", colour = \"coral4\", bins = 10) +\n  theme_minimal()\n\n\n\n\n\n\n\nRerun the code above a few times. What do you notice about the boxplots and histograms that you are generating?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#lab-3.2-conceptual-confidence-intervals",
    "href": "conf-int.html#lab-3.2-conceptual-confidence-intervals",
    "title": "3  Confidence Intervals",
    "section": "Lab 3.2: Conceptual Confidence Intervals",
    "text": "Lab 3.2: Conceptual Confidence Intervals\nThe primary purpose of this lab is to explore what the concept of “confidence” really means. To do so, we will construct confidence intervals first for a population mean (\\(\\mu\\)) using the “one-sample t” confidence interval formula .\nOne Sample t (CI for \\(\\mu\\))\nIf we were to repeatedly generate random samples from a population and use those samples to construct (1 - \\(\\alpha\\))% confidence intervals, the coverage rate is defined as the fraction of those intervals that actually contain (capture) the parameter.\nIn the code below, we explore the average width and the coverage rate of confidence intervals for \\(\\mu\\) using the “standard” one-sample t formula.\n\ngenerate_onesamp_cis &lt;- function(n, mu, sigma, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x &lt;- rnorm(n, mu, sigma)\n  \n  ## compute the bounds of the ci\n  point_est &lt;- mean(x)\n  lb &lt;- point_est - qt(1 - alpha/2, df = n - 1) * sd(x) / sqrt(n)\n  ub &lt;- point_est + qt(1 - alpha/2, df = n - 1) * sd(x) / sqrt(n)\n  \n  ## put everything into a tibble\n  out_df &lt;- tibble(point_est, lb, ub)\n  \n  return(out_df)\n}\n\n\n## define parameters to use in our function\nn &lt;- 5   # sample size\nmu &lt;- 10     # true mean\nsigma &lt;- 5    # true standard deviation\nalpha &lt;- 0.05  # used to construct 1-alpha CI (how much area should be in the tails)\n\n## generate one sample and one ci\ngenerate_onesamp_cis(n = n, mu = mu, sigma = sigma, alpha = alpha)\n\nBut, if we want to explore the coverage rate and average interval width, we need to simulate many confidence intervals. Below, we map() through our function nsim times and bind the results together into a data frame at the end:\n\nnsim &lt;- 1000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, mu = mu,\n                                            sigma = sigma,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 1,000 × 3\n   point_est       lb    ub\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1      9.69  4.34    15.0 \n 2      8.88  0.00168 17.8 \n 3      4.85  0.0440   9.65\n 4     11.9   8.18    15.7 \n 5      9.38  3.93    14.8 \n 6     12.5   6.26    18.7 \n 7     11.9   9.49    14.4 \n 8     11.3   6.02    16.5 \n 9      8.16 -1.25    17.6 \n10      5.06 -2.58    12.7 \n# ℹ 990 more rows\n\n\nFinally, since we are interested in average interval width and coverage rate, we can create variables for the width of each confidence interval and for whether or not each interval “covers” the true mean mu:\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(mu &gt; lb & mu &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nAnd we can then summarise() to get the average width and the coverage rate:\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1      11.7         0.957\n\n\nExercise.\nNote that if you wanted to compare the performance of the one-sample t formula using Normal data and non-Normal data, it is important that you generate data from those populations in such a way that the populations have the same means and standard deviations (otherwise the comparisons would not be fair - especially any comparisons of average width!). Here are some guidelines about how you might do that.\nAn Exponential Population: We know for an Exponential random variable \\(Y\\) that \\(E(Y) = 1 / \\lambda\\) and \\(Var(Y) = 1 / \\lambda^2 = (1 / \\lambda)^2\\). We notice here, that for Exponential random variables, the variance is the mean squared (which implies that the standard deviation equals the mean). Thus, if you wanted to compare the performance of the one-sample t formula for Normal and Exponential data, you would need to focus exclusively on situations where \\(mu = sigma\\) and you could identify which value of \\(lambda\\) is necessary to achieve your desired mean.\nChi-square would be another distribution where there is a pretty easy relationship between mean and standard deviation (and therefore choose the df for Chi-Square). Gamma is also reasonble, but does require a little bit of algebra to identify the \\(\\alpha\\) and \\(\\lambda\\) needed to get the desired mean and standard deviation.\nTwo Sample t\nNext, we suppose that we have two populations and that we are interested in constructing a confidence interval for the difference in population means, \\(\\mu_1 - mu_2\\). To do so, we will not assume that the underyling variances of each population are equal and will instead use The Welch-Satterthwaite approximation to degrees of freedom for the \\(t^*\\) value to create the confidence interval.\n\nn1 &lt;- 15         # size of first sample\nmu1 &lt;- 20        # mean of first population\nsigma1 &lt;- 5      # standard deviation of first population\n\nn2 &lt;- 15         # size of second sample\nmu2 &lt;- 15        # mean of second population\nsigma2 &lt;- 5      # standard deviation of second population\n\nx &lt;- rnorm(n1, mu1, sigma1)       # generate sample 1 from Population 1\ny &lt;- rnorm(n2, mu2, sigma2)       # generate sample 2 from Population 2\n\nvarest1 &lt;- var(x)\nvarest2 &lt;- var(y)\n\nA &lt;- varest1 / n1\nB &lt;- varest2 / n2\n\nnumerator &lt;- (A + B) ^2\ndenominator &lt;- A^2 / (n1 - 1) + B^2 / (n2 - 1)\n\nwelchdf &lt;- numerator / denominator\nwelchdf\n\nGenerating Sample Proportions\nFinally, if we are interested in constructing a confidence interval for a population proportion instead of a population mean (so, our variable of interest is categorical with two levels), then we can use the following code to generate a sample proportion from a population with a true proportion of success \\(p\\) and a certain sample size \\(n\\):\n\nn &lt;- 10   # sample size\np &lt;- 0.5  # population proportion\n  \nx &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n## number of successes divided by sample size\nphat &lt;- x / n\nphat",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#lab-3.3-bootstrap-confidence-intervals",
    "href": "conf-int.html#lab-3.3-bootstrap-confidence-intervals",
    "title": "3  Confidence Intervals",
    "section": "Lab 3.3: Bootstrap Confidence Intervals",
    "text": "Lab 3.3: Bootstrap Confidence Intervals\nThe purpose of this lab is to construct bootstrap confidence intervals for a population mean, a population proportion, and for a difference in population means. To start, we construct a bootstrap confidence interval for a population mean.\nBootstrap CI for a Population Mean\nBelow, the number of hours of exercise per week for a random sample of Stat 113 students are provided.\n\nlibrary(tidyverse)\nstat113_ex &lt;- tibble(Exercise = c(12, 3, 4, 10, 8, 17, 15, 5, 8, 10, 8,\n                                  25, 1, 15, 2, 14, 10, 8, 6, 14, 5, 6,\n                                  12, 3))\n\n# for demonstrating danger of bootstrapping with small sample sizes\n#stat113_ex = tibble(Exercise = c(25, 1, 2, 4, 0, 3, 0))\n\nBefore constructing any confidence interval, we shoud create a plot of the data and obtain the sample mean:\n\nggplot(data = stat113_ex, aes(x = Exercise)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 10) +\n  theme_minimal()\n\n\n\n\n\n\nmean(stat113_ex$Exercise) # sample mean\n\n[1] 9.208333\n\n\nNext, we create the bootstrap distribution of sample means and make a histogram of that distribution. For this example, we generate 1000 bootstrap samples. The function below outputs one bootstrap mean from a single bootstrap sample:\n\ngenerate_one_bootmean &lt;- function() {\n  ## put exercise variable into a vector\n  exercise_vec &lt;- stat113_ex |&gt; pull(Exercise)\n  \n  ## sample the exercise data with replacement\n  boot_samp &lt;- sample(exercise_vec, size = length(exercise_vec),\n                      replace = TRUE)\n  \n  boot_mean &lt;- mean(boot_samp)\n  \n  return(boot_mean)\n}\n\ngenerate_one_bootmean()\n\nWe can then map over the function to obtain B bootstrap means:\n\nB &lt;- 2000\nbootmean_vec &lt;- map_dbl(1:B, \\(i) generate_one_bootmean())\n\nWith our 2000 bootstrap means, we can construct a plot of the bootstrap distribution. This distribution should be somewhat centered at the sample mean.\n\nbootmean_df &lt;- tibble(bootmean_vec)\nggplot(data = bootmean_df, aes(x = bootmean_vec)) +\n  geom_histogram(colour = \"black\", fill = \"mediumseagreen\", bins = 14) +\n  geom_vline(xintercept = mean(stat113_ex$Exercise),\n             colour = \"grey\", linewidth = 2) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe can also compute some quantities of interest that we discussed in class:\n\n## mean of bootstrap distribution (should be close to our statistic)\nmean(bootmean_vec)\n\n## Bootstrap SE\nsd(bootmean_vec)\n\n# bootstrap estimate of bias\nbias &lt;- mean(bootmean_vec) - mean(stat113_ex$Exercise)\nbias\n\n# what percent of the BS variation is due to bias?\nabs(bias) / sd(bootmean_vec)\n\n## Percentile Bootstrap confidence intervals\n# 90% CI\nquantile(bootmean_vec, c(0.05, 0.95))\n# 95% CI\nquantile(bootmean_vec, c(0.025, 0.975))\n\nBootstrap CI for a Population Proportion\nIn a random sample of 588 adults, 16.67% reported that key lime pie is their favorite kind of pie (only 16.67%….key lime is the GOAT of pies). Construct a 95% Bootstrap percentile confidence interval for the proportion of all adults that have key lime as their favorite pie.\n\n## create a data frame where a 1 corresponds to a person who\n## chose key lime pie while a 0 corresponds to a person who\n## chose anything else other than key lime pie\n\nn &lt;- 588\np_hat &lt;- 0.1667\npie_df &lt;- tibble(key_lime_fav = c(rep(1, round(n * p_hat)),\n                                  rep(0, n - round(n * p_hat))))\n\n## sample proportion (double check)\nmean(pie_df$key_lime_fav)\n\n[1] 0.1666667\n\n\nFirst, we write a function to obtain a single bootstrap proportion from a single bootstrap sample:\n\ngenerate_one_bootprop &lt;- function() {\n  ## put exercise variable into a vector\n  pie_vec &lt;- pie_df |&gt; pull(key_lime_fav)\n  \n  ## sample the exercise data with replacement\n  boot_samp &lt;- sample(pie_vec, size = length(pie_vec),\n                      replace = TRUE)\n  \n  boot_prop &lt;- mean(boot_samp)\n  \n  return(boot_prop)\n}\n\ngenerate_one_bootprop()\n\n[1] 0.164966\n\n\nWe can then map over the function to obtain B bootstrap proportions and put these bootstrap proportions into a data frame:\n\nB &lt;- 5000\nbootprop_vec &lt;- map_dbl(1:B, \\(i) generate_one_bootprop())\n\nbootprop_df &lt;- tibble(bootprop_vec)\n\nExercise. Construct a histogram of the bootstrap sample proportions, adding in a vertical line for the sample proportion.\nExercise. Compute quantities of interest to help construct a percentile confidence interval for the true proportion. You should compute: the center of the bootstrap distribution, the bootstrap standard error, the bootstrap estimate of bias, the percent of the boostrap variation that is due to bias, a 90% percentile-based confidence interval for the true proportion, and a 95% percentile-based confidence interval for the true proportion.\nExercise. Interpret the 90% confidence interval in context of the problem.\nBootstrap CI’s with Two (Independent) Samples\nFinally, we construct a bootstrap confidence interval using two independent random samples. We are interested in two different quantities here: the difference in population means and the ratio of population means. To do so, we use the flight data, which has information on flight delays from a sample of United Airlines flights (UA) and a sample of Amaerican Airlines flights (AA). We are interested in if either of these airlines has a higher average delay than the other.\nFirst, we obtain some summary statistics and make a plot of the data:\n\nlibrary(resampledata)\ndelay_df &lt;- FlightDelays |&gt; as_tibble()\n## set.seed(13617)  # 50100 for bias illustration\n\ndelay_df |&gt; group_by(Carrier) |&gt;\n  summarise(n = n(),\n            xbar = mean(Delay),\n            sd = sd(Delay))\n\n# A tibble: 2 × 4\n  Carrier     n  xbar    sd\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 AA       2906  10.1  40.1\n2 UA       1123  16.0  45.1\n\nggplot(delay_df,\n       aes(x = Carrier, y = Delay)) + \n  geom_boxplot(fill = \"steelblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\nOur sample statistic for the difference in means and for the ratio of means can be calculated with:\n\ndelays_aa &lt;- delay_df |&gt; filter(Carrier == \"AA\")\ndelays_ua &lt;- delay_df |&gt; filter(Carrier == \"UA\")\n\ndiffmeans &lt;- mean(delays_aa$Delay) - mean(delays_ua$Delay) # our sample statistic (diff. in means)\nratiomeans &lt;- mean(delays_aa$Delay) / mean(delays_ua$Delay) # our sample statistic (ratio of means)\n\nThe code below creates a function that generates either the boostrap difference in means or the bootstrap ratio of means from a single bootstrap sample:\n\ngenerate_one_boot &lt;- function(statistic = \"diff\") {\n  ## statistic can also be \"ratio\"\n  \n  ## put exercise variable into a vector\n  aa_vec &lt;- delays_aa |&gt; pull(Delay)\n  ua_vec &lt;- delays_ua |&gt; pull(Delay)\n  \n  ## sample the exercise data with replacement\n  boot_samp &lt;- sample(pie_vec, size = length(pie_vec),\n                      replace = TRUE)\n  \n  boot_prop &lt;- mean(boot_samp)\n  \n  ## FILL IN MISSING PIECES\n\n  ## fill in piece to obtain one bootstrap sample mean for aa flights\n\n  ## fill in piece to obtain one bootstrap sample mean for ua flights\n  \n  if (statistic == \"diff\") {\n    \n    ## fill in piece to compute the difference in sample means\n    \n    return(boot_diff)\n    \n  } else if (statistic == \"ratio\") {\n    \n    ## fill in piece to compute the ratio of sample means\n    \n    return(boot_ratio)\n  }\n  \n}\n\nExercise. Using this function, we can obtain the bootstrap distribution of differences and the bootstrap distribution of ratios. Use the previous code to construct the histograms of these two bootstrap distributions, overlaying the sample statistics for each onto their appropriate bootstrap distributions.\nI’ve provided code to generate the bootstrap distribution of differences in means but you will need to adjust this to also create the bootstrap distribution for the ratio.\n\nB &lt;- 5000\nbootdiff_vec &lt;- map_dbl(1:B, \\(i) generate_one_boot(statistic = \"diff\"))\n\nbootdiff_df &lt;- tibble(bootdiff_vec)\n\nExercise. Compute relevant quantities of interest to help create a 90% percentile-based bootstrap confidence interval for the true mean difference in delay times and the true ratio of delay times.\nExercise. Interpret your two bootstrap confidence intervals in context of the problem.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "conf-int.html#mini-project-3-simulation-to-investigate-confidence-intervals",
    "href": "conf-int.html#mini-project-3-simulation-to-investigate-confidence-intervals",
    "title": "3  Confidence Intervals",
    "section": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "text": "Mini Project 3: Simulation to Investigate Confidence Intervals\nProject Description\nAI Usage: You may not use generative AI for this project in any way.\nCollaboration: For this project, you may work with a self-contained group of 3. Keep in mind that you may not work with the same person on more than one mini-project (so, if you worked with a student on the first mini-project as a partner or in a small group, you may not work with that person on this project). Finally, if working with a partner or in a group of 3, you may submit the same code and the same table of results, but your write-up (both the short summary of your methods and your findings summary) must be written individually.\nStatement of Integrity: At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.\n“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\n\nFor this Mini-Project, you will design a simulation study to explore confidence. Some possible study design ideas appear below. I have also included a small “bad” example.\nIn your simulation study, you will need to choose one or more confidence interval formulas (e.g., a one-sample t test, or comparing the different versions of the two-sample t confidence interval formula). Then, you will need to decide the conditions under which you want to evaluate your methods (e.g., different samples sizes, or when conditions are met or not, and/or possibly exploring how severely the conditions are not met). In your design, you should consider at least six different cases/scenarios to explore (note in my “bad” example, I explored 9 cases – all of the possible combinations of \\(n\\) and \\(\\sigma\\)).\nPossible Study Designs\n\n\nCompare the different Two-Sample t formulas (including STAT 113 df versus Welch df, or Welch formula versus Pooled variance, or all three) under different conditions (sample sizes and population variances).\nCompare the different one proportion formulas under different conditions (sample sizes and population proportions, and/or conditions met or not).\nCompare the different two proportion formulas under different conditions (sample sizes and population proportions, and/or conditions met or not).\n\nWhat to Submit: You should submit a typed write-up of your simulation results. There are a few possible formats you might use to submit this mini-project:\n\na rendered .qmd file (either to .html, .pdf, or .docx) that includes your write-up, results, and code.\na Word file that contains your results and write-up and a separate .qmd file that has all of your code. Note that, for this option, your code should still be organized and easy to follow.\n\nIn your submission, you should include:\n\na short summary of the methods you are comparing (including the assumptions necessary for those methods) and under what conditions.\na table that summarises the coverage rates and average widths for the cases you considered.\ntwo paragraphs summarising your findings.\nSmall (Bad) Example\nBelow I give a small “bad” example for a one-sample t confidence interval simulation (a setting that you cannot choose for this project anyway!).\nI plan to compare the performance of the one-sample t confidence interval for different sample sizes (\\(n = 5, 25, 100\\)) and different population standard deviations (\\(\\sigma= 2, 10, 20\\)). Note that one-sample t methods assume that either data come from a Normal population or we have a large sample size. For this simulation, all data will be randomly generated from a Normal population with mean \\(\\mu = 10\\). And, for each setting, I simulated 5000 iterations to compute the coverage rate and average width. A summary of my simulated coverage rates for these combinations appears below.\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(n = 5\\)\n\\(n = 25\\)\n\\(n = 100\\)\n\n\n\n\\(\\sigma = 2\\)\nCoverage Rate\n0.947\n0.952\n0.965\n\n\n\\(\\sigma = 2\\)\nAverage Width\n4.59\n1.64\n0.79\n\n\n\n\n\n\n\n\n\n\\(\\sigma = 10\\)\nCoverage Rate\n0.949\n0.935\n0.95\n\n\n\\(\\sigma = 10\\)\nAverage Width\n23.26\n8.19\n3.93\n\n\n\n\n\n\n\n\n\n\\(\\sigma = 20\\)\nCoverage Rate\n0.939\n0.953\n0.951\n\n\n\\(\\sigma = 20\\)\nAverage Width\n46.68\n16.29\n7.91\n\n\n\nI have learned that, for each combination of sample size and population standard deviation, the coverage rate is right around what was expected (95%). The average width is larger when either \\(n\\) is small or the population variance is large. The average width decreases when either sample size increases or the population standard deviation decreases.\nNote that the reason I’ve labeled this example as “bad” is that I haven’t done anything that could possibly affect the coverage rate; I only modified factors that would influence the width of the confidence intervals (specifically factors that would affect the standard error of the confidence interval and nothing else). More interesting simulation designs would explore what happens when you use a formula when you are not supposed to (e.g., conditions fail or you use a rule for degrees of freedom that I’ve said not to use or use a formula that I’ve said you should not use).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "4  Bayesian Statistics",
    "section": "",
    "text": "Lab 4.1: Introduction to Bayesian: Binomial Data\nIn this lab, we will analyze binomial data in a Bayesian framework with a couple of different prior distributions for \\(p\\). The first prior for \\(p\\) will be a non-informative prior while the second prior for \\(p\\) will be based on “expert” (your!) opinion prior to the analysis.\nTo start, consider your own prowess at the popular game “flip cup” and your (self-assessed) probability of successfully flipping the cup so that it’s upside-down. Use the app at http://shiny.stlawu.edu:3838/sample-apps/stat325/distplot/ to drag the sliders around until you settle on a reasonable informative prior distribution for the probability that you successfully flip a cup from right-side-up to upside-down (note that you should not use a non-informative prior here). In general:\nWrite down the parameters you will use for your informative prior of you successfully flipping a cup.\nIn comparison, we will also use a Beta(1, 1) = Uniform(0, 1) prior distribution for \\(p\\). This is a non-informative prior.\nAfter you have settled on your informative prior, use the code below to construct a plot of both the informative prior and the non-informative prior.\nlibrary(tidyverse)\nps &lt;- seq(0, 1, length.out = 1000)\n\ninformative_alpha &lt;- 2\ninformative_beta &lt;- 4\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_prior &lt;- dbeta(ps, informative_alpha,\n                           informative_beta)\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\nprior_plot &lt;- tibble(ps, informative_prior, noninformative_prior) |&gt;\n  pivot_longer(2:3, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\nExercise. With our informative prior and our non-informative prior, we will now collect some data to include in our bayesian analysis. After you collect data, derive the posterior distribution of \\(p\\) using both the noninformative prior and the informative prior. Then, adjust the code above to produce a plot of your two posterior distributions.\n## put code to make your posterior distribution plot here\nExercise. For each posterior, compute the posterior mean.\nExercise. For each posterior, compute a 95% credible interval for \\(p\\).\nExercise. The informative prior that you used was very subjective, based on your own knowledge and thoughts of how well you can play flip cup. What do you think can be done to limit the subjectivity of this prior?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayesian.html#lab-4.1-introduction-to-bayesian-binomial-data",
    "href": "bayesian.html#lab-4.1-introduction-to-bayesian-binomial-data",
    "title": "4  Bayesian Statistics",
    "section": "",
    "text": "increasing \\(\\alpha\\) will shift the distribution to the right.\nincreasing \\(\\beta\\) will shift the distribution to the left.\nincreasing both will give a distribution with less variability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayesian.html#lab-4.2-more-bayesian-poisson-data",
    "href": "bayesian.html#lab-4.2-more-bayesian-poisson-data",
    "title": "4  Bayesian Statistics",
    "section": "Lab 4.2: More Bayesian: Poisson Data",
    "text": "Lab 4.2: More Bayesian: Poisson Data\nRecall from probability that we discussed using a Poisson model to model the number of goals that the St. Lawrence women’s hockey team scores in a game. In that class, we assumed that we knew the value of \\(\\lambda\\) in that probability model and we computed a few quantities of interest under that assumption.\nUsually, however, \\(\\lambda\\) must be estimated (in a frequentist analysis) using data, or, in a Bayesian analysis, we can provide a probability model for \\(\\lambda\\) itself that is either non-informative or informed by expert opinion or prior data.\nSuppose that the women’s hockey coach says that he thinks their team scores about 3 goals per game, on average. When pressed for how “sure” they are of that answer and to give a reasonable range for what values that goal scoring rate is, they reply that they are not quite sure. But, they know that they are almost certain that the scoring rate is no less than 2 goals per game, on average.\nAs discussed in class, the conjugate prior for Poisson data is the Gamma distribution, which can be used to model \\(\\lambda\\). Using the coach’s information, come up with an informative prior for \\(\\lambda\\) with the Gamma distribution. Note that we will have some decisions to make in how to use the coach’s information to come up with an informative prior!\n\n## put work for informative prior here\n\nA relatively non-informative prior for \\(\\lambda\\) is a Gamma distribution with very small values for both \\(\\alpha\\) and \\(k\\). Note that, for the gamma distribution, I am replacing \\(\\lambda\\) with \\(k\\), so that we are not working with two different \\(\\lambda\\)’s (since the Poisson also has a parameter called \\(\\lambda\\)). Using \\(\\alpha = 0.001\\) and \\(k = 0.001\\), construct a plot of a relatively non-informative prior for \\(\\lambda\\) using the code below.\n\nalpha &lt;- 0.001\nk &lt;- 0.001\nlambda_grid &lt;- seq(0, 5, length.out = 1000)\ngamma_density &lt;- dgamma(lambda_grid, shape = alpha, rate = k)\ngamma_plot &lt;- tibble(lambda_grid, gamma_density)\nggplot(data = gamma_plot, aes(x = lambda_grid, y = gamma_density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise. Based on the plot of the non-informative prior, it’s a little difficult to see why this prior is non-informative. However, using what we derived as the posterior distribution for \\(\\lambda\\), construct an argument for why a prior of \\(\\alpha = 0.001\\) and \\(k = 0.001\\) is a non-informative prior.\nExercise. Using the code above, construct a plot of the informative prior we derived earlier.\nNow suppose that we collect data from the women’s hockey team this season to update both our non-informative prior and our informative prior with data to obtain two posterior distributions for the goal rate.\nThe code below pulls in data from the season:\n\nlibrary(rvest)\nurl &lt;- \"https://saintsathletics.com/sports/womens-ice-hockey/stats/2024-25\"\ntab &lt;- read_html(url) |&gt; \n  html_nodes(\"table\")\nhockey_stats &lt;- tab[[6]] |&gt; html_table(header = FALSE) \n\nnewnames &lt;- paste(hockey_stats[1, ], hockey_stats[2, ])\ngoals &lt;- hockey_stats |&gt; set_names(newnames) |&gt;\n  slice(-1, -2) |&gt;\n  mutate(`Shots G` = as.numeric(`Shots G`)) |&gt;\n  filter(`Shots G` &lt;= 20) |&gt; ## filter out the totals (hoping that the women\n## never scored more than 20 goals in one game!!)\n  pull(`Shots G`) |&gt; as.numeric()\ngoals\n\nExercise. Using this data and the posterior that we computed in class, figure out the posterior distribution for the goal rate with the non-informative prior and with the informative prior.\nExercise. Construct a plot of each of the posterior distributions.\nExercise. The mean of the posterior distribution must always be between the mean of the prior distribution and the mean of the data, as the posterior is a “compromise” between the prior and the observed data. Verify that this is the case for this example.\nExercise. With each posterior, compute a 95% credible interval for \\(\\lambda\\), the rate that the women’s hockey team scores goals.\nExercise. Think back to the data that we used for this example. What assumptions have we made to complete this analysis? Can you think of ways that we might relax these assumptions?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayesian.html#mini-project-4-bayesian-analysis",
    "href": "bayesian.html#mini-project-4-bayesian-analysis",
    "title": "4  Bayesian Statistics",
    "section": "Mini Project 4: Bayesian Analysis",
    "text": "Mini Project 4: Bayesian Analysis\nProject Introduction\nAI Usage: You may not use generative AI for this project in any way.\nCollaboration: For this project, you may work with a self-contained group of 3. Keep in mind that you may not work with the same person on more than one mini-project (so, if you worked with a student on the first or third mini-project as a partner or in a small group, you may not work with that person on this project). Finally, if working with a partner or in a group of 3, you may submit the same code and the same results/visuals, but your write-ups must be written individually.\nStatement of Integrity: At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.\n“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\n\nRafael Nadal is arguably the greatest men’s clay-court tennis player ever to play the game. In this mini-project, you analyze the probability that Nadal wins a point on his own serve against his primary rival, Novak Djokovic, at the French Open (the most prestigious clay court tournament in the world).\nPriors\nBefore we look at some data, we will consider a few possible prior distributions for the probability that Nadal wins a point on his own serve against Djokovic:\n\nnon-informative prior for this probability.\nan informative prior based on a clay-court match the two played in the previous year. In that match, Nadal won 46 out of 66 points on his own serve. The standard error of this estimate is 0.05657.\nan informative prior based on a sports announcer, who claims that they think Nadal wins about 75% of the points on his serve against Djokovic. They are also “almost sure” that Nadal wins no less than 70% of his points on serve against Djokovic.\n\nConstruct a single graph that shows all three of these priors. Note that, for both of the informative priors, there is some subjectivity with how you are going to use the information given to construct an appropriate prior for \\(p\\), the probability that Nadal wins a point on serve against Djokovic.\nData\nNow, we want to use the 2020 French Open data to update our prior for the probability that Nadal wins a point on serve. In that tournament, the two players played in the final. In that final, Nadal served 84 points and won 56 of those points.\nUpdate each of your three priors with this data and make a plot of the three different posterior distributions of \\(p\\).\nAdditionally, for each of the three posteriors, report the posterior mean and 90% credible intervals for \\(p\\).\nReport\nFor this mini-project, you should submit a report that includes the following:\n\nan introduction to the question of interest that you are answering for this project as well as an overview of how you will be answering this question.\ntyped work justifying the decisions you made to create the two informative priors. What did you assume when you made those priors?\nthe graph of the three priors and the graph of the three posteriors, along with any work needed to obtain the posteriors, the posterior means, and the 90% credible intervals\na comparison of the three posteriors.\n\nThey should be a bit different from each other: why?\nIf you had to choose one to use, which one would you choose here and why?\nThe variance of each posterior should be different. Why do you think one posterior has a lower variance than the other two?\n\n\na brief conclusion of what you found in this mini-project.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "hyp-test.html",
    "href": "hyp-test.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "Lab 5.1: Permutation Tests",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hyp-test.html#lab-5.1-permutation-tests",
    "href": "hyp-test.html#lab-5.1-permutation-tests",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "Example 1: Flight Data\nConsider again the flight delay data. Here, we will consider whether or not there is statistical evidence that the mean delay of United Airlines is different than the mean delay of American Airlines.\nRecal that, for a permutation test, we do not need to assume that the underlying populations follow a normal distribution. However, if we are interested in testing a difference in means, we do need to assume that the populations have the same variance and the same shape (but perhaps may have a different center).\nFirst, we obtain some summary statistics and make a plot of the data:\n\nlibrary(tidyverse)\nlibrary(resampledata)\ndelay_df &lt;- FlightDelays |&gt; as_tibble() |&gt;\n  select(Carrier, Delay, everything())\n## set.seed(13617)  # 50100 for bias illustration\n\ndelay_df |&gt; group_by(Carrier) |&gt;\n  summarise(n = n(),\n            xbar = mean(Delay),\n            sd = sd(Delay))\n\n# A tibble: 2 × 4\n  Carrier     n  xbar    sd\n  &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 AA       2906  10.1  40.1\n2 UA       1123  16.0  45.1\n\nggplot(delay_df,\n       aes(x = Carrier, y = Delay)) + \n  geom_boxplot(fill = \"steelblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\nWe next store the difference in sample means as a value called teststat and each of the sample sizes as n_a and n_u:\n\nteststat &lt;- delay_df |&gt; group_by(Carrier) |&gt;\n  summarise(mean_delay = mean(Delay)) |&gt;\n  pull(mean_delay) |&gt;\n  diff()\n## teststat is united delay mean minus american airlines mean\n\nUnder the null hypothesis, the distribution of delays is identical for american and united airlines. To reshuffle that delay values randomly across both airlines we can use the code below:\n\ndelay_df |&gt;\n  mutate(delay_perm = sample(Delay, size = nrow(delay_df), replace = FALSE)) |&gt;\n  relocate(delay_perm)\n\n# A tibble: 4,029 × 11\n   delay_perm Carrier Delay    ID FlightNo Destination DepartTime Day   Month\n        &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt;    &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;\n 1         40 UA         -1     1      403 DEN         4-8am      Fri   May  \n 2          0 UA        102     2      405 DEN         8-Noon     Fri   May  \n 3         -6 UA          4     3      409 DEN         4-8pm      Fri   May  \n 4         -3 UA         -2     4      511 ORD         8-Noon     Fri   May  \n 5         -1 UA         -3     5      667 ORD         4-8am      Fri   May  \n 6          2 UA          0     6      669 ORD         4-8am      Fri   May  \n 7         -2 UA         -5     7      673 ORD         8-Noon     Fri   May  \n 8          0 UA          0     8      677 ORD         8-Noon     Fri   May  \n 9         -6 UA         10     9      679 ORD         Noon-4pm   Fri   May  \n10         -5 UA         60    10      681 ORD         Noon-4pm   Fri   May  \n# ℹ 4,019 more rows\n# ℹ 2 more variables: FlightLength &lt;int&gt;, Delayed30 &lt;fct&gt;\n\n\nRerun the previous code chunk a few times to see a different permutations of the Delay variable.\nWith one permutation, we want to recalculate the difference in means:\n\ndelay_df |&gt;\n  mutate(delay_perm = sample(Delay, size = nrow(delay_df), replace = FALSE)) |&gt;\n  relocate(delay_perm) |&gt;\n  group_by(Carrier) |&gt;\n  summarise(mean_delay_perm = mean(delay_perm)) |&gt;\n  pull(mean_delay_perm) |&gt;\n  diff()\n\n[1] -0.279834\n\n\nRerun the code chunk above a few times to get a few different mean differences under different permutations. The big idea is that a large number of these mean differences will form the null distribution for the difference in sample means (if there really is no difference in the underlying population distributions).\nSo, let’s wrap that above code in a function, and then iterate over that function a large number of times to obtain our null distribution:\n\nget_delay_perm_diff &lt;- function() {\n  delay_df |&gt;\n    mutate(delay_perm = sample(Delay, size = nrow(delay_df), replace = FALSE)) |&gt;\n    relocate(delay_perm) |&gt;\n    group_by(Carrier) |&gt;\n    summarise(mean_delay_perm = mean(delay_perm)) |&gt;\n    pull(mean_delay_perm) |&gt;\n    diff()\n}\n\nn_perm &lt;- 5000\ndiff_means &lt;- map_dbl(1:n_perm, \\(i) get_delay_perm_diff())\nnull_df &lt;- tibble(diff_means)\n\nggplot(data = null_df, aes(x = diff_means)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise. Write a 1-2 sentence interpretation of what this null distribution means in context of the problem.\nNow, let’s add where our test statistic from our data is on the graph:\n\nggplot(data = null_df, aes(x = diff_means)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  geom_vline(xintercept = teststat, colour = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\nExercise. Before explicitly calculating a p-value, assess visually whether a p-value for this hypothesis test will be relatively large or relatively small.\nNow, let’s explicitly compute a conservative p-value for the test:\n\n(sum(diff_means &gt; abs(teststat)) + sum(diff_means &lt; -abs(teststat)) + 1) / (n_perm + 1)\n\n[1] 0.00019996\n\n\nExercise. What is the alternative hypothesis that the p-value is testing?\nExercise. What is the purpose of the + 1 in the code above?\nExercise. Why are there two different summations in the code above to get the p-value?\nExercise. Write a conclusion in context of the problem for this hypothesis test.\nExample 2: Spruce\n(might change this so that the resulting p-value is not so small).\nIn the Black Spruce Case Study in Section 1.9, seedlings were planted in plots that were either subject to competition (from other plants) or not. Use the data set Spruce to conduct a test to see if there is evidence that the mean growth (Ht.change) differs for the two treatments (C = Competition or NC = No Competition).\n\n## print as a tibble\nspruce_df &lt;- Spruce |&gt; as_tibble()\n\nData Exploration:\n\nggplot(data = spruce_df, aes(x = Competition, y = Ht.change, fill = Competition)) +\n  geom_boxplot(outlier.shape = 8) +\n  theme_minimal() +\n  scale_fill_viridis_d(begin = 0.4, end = 0.9) +\n  guides(fill = \"none\")\n\n\n\n\n\n\nspruce_df |&gt; group_by(Competition) |&gt;\n  summarise(mean = mean(Ht.change),\n            n = n())\n\n# A tibble: 2 × 3\n  Competition  mean     n\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 C            25.7    36\n2 NC           36.2    36\n\n\nTest Statistic:\n\n# From our summary statistics\nteststat &lt;- spruce_df |&gt; group_by(Competition) |&gt;\n  summarise(mean = mean(Ht.change),\n            n = n()) |&gt;\n  pull(mean) |&gt;\n  diff()\n## NC minus C\nteststat\n\n[1] 10.46111\n\n\nCreate Null Distribution:\n\nget_spruce_perm_diff &lt;- function() {\n  spruce_df |&gt;\n    mutate(height_perm = sample(Ht.change,\n                                size = nrow(spruce_df), replace = FALSE)) |&gt;\n    relocate(height_perm) |&gt;\n    group_by(Competition) |&gt;\n    summarise(mean_height_perm = mean(height_perm)) |&gt;\n    pull(mean_height_perm) |&gt;\n    diff()\n}\nget_spruce_perm_diff()\n\n[1] 4.65\n\nn_perm &lt;- 5000\ndiff_means &lt;- map_dbl(1:n_perm, \\(i) get_spruce_perm_diff())\nnull_df &lt;- tibble(diff_means)\n\nggplot(data = null_df, aes(x = diff_means)) +\n  geom_histogram(colour = \"black\", fill = \"white\", bins = 15) +\n  ## add test stat to null dist\n  geom_vline(xintercept = teststat, colour = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCalculate a p-value:\n\n(sum(diff_means &gt; abs(teststat)) + sum(diff_means &lt; -abs(teststat)) + 1) / (n_perm + 1)\n\n[1] 0.00019996\n\n\nExercise. Write a conclusion in context of the problem for this hypothesis test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hyp-test.html#mini-project-5-advantages-and-drawbacks-of-using-p-values",
    "href": "hyp-test.html#mini-project-5-advantages-and-drawbacks-of-using-p-values",
    "title": "5  Hypothesis Testing",
    "section": "Mini Project 5: Advantages and Drawbacks of Using p-values",
    "text": "Mini Project 5: Advantages and Drawbacks of Using p-values\nAI Usage: You may not use generative AI for this project in any way.\nCollaboration: For this project, you may not work with other students in the class.\nStatement of Integrity: At the top of your submission, copy and paste the following statement and type your name, certifying that you have followed all AI and collaboration rules for this mini-project.\n“All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project.”\n\nFor this final mini-project, you will read an editorial published in The American Statistician and respond to the questions below. You will find a pdf of the editorial on Canvas.\nFirst, read through the first six sections of the editorial titled Moving to a World Beyond ‘p&lt;0.05’ by Wasserstein, Schirm, and Lazar published in The American Statistician in 2019 as the introduction to a special issue about p-values. I strongly encourage you to read the first six sections at least twice before you attempt to respond to the questions that appear below. This mini-project will be graded, primarily, on how thoughtfully you respond to these questions.\nSubmission: Upload your typed responses to these questions to the assignment in Canvas by the deadline.\nQuestions:\n\nTowards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\nSection 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of \\(p\\); in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\nSection 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\nSection 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\nSection 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\nSection 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\nFind a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "6  Portfolio",
    "section": "",
    "text": "Information about the final portfolio will be added here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Portfolio</span>"
    ]
  }
]